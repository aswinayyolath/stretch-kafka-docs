{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Stretch Cluster using Strimzi Apache Kafka is widely used for high-throughput, real-time data streaming, but deploying it across multiple Kubernetes clusters presents unique challenges. A Stretch Kafka Cluster is a deployment model where Kafka brokers and controllers are distributed across multiple Kubernetes clusters while maintaining a single logical Kafka cluster. This enables improved fault tolerance, scalability, and disaster recovery while keeping latency low. This documentation provides step-by-step guidance on setting up a Stretch Kafka Cluster using Submariner or Cilium for cross-cluster networking. The goal is to enable seamless communication between Kafka brokers and controllers running in different Kubernetes clusters, ensuring high availability and resilience. Note \ud83d\udea8 This not an offically supported Strimzi feature or documentation.\ud83d\udea8 Overview of stretch Kafka clusters What is a stretch Kafka cluster? A Stretch Kafka Cluster extends a single Kafka deployment across multiple Kubernetes clusters. Unlike a standard Kafka deployment, which runs within a single Kubernetes cluster, a stretch cluster allows brokers and controllers to be distributed across multiple clusters while functioning as a unified Kafka instance. Key benefits of stretch Kafka clusters High Availability and Disaster Recovery : If one Kubernetes cluster fails, Kafka continues to operate using nodes in the remaining K8s clusters. Scalability : Kafka brokers can be added across multiple K8s clusters to handle increasing workloads. Optimized Workload Distribution : Workloads can be balanced across different K8s clusters for improved resource utilization. Deployment considerations Low-Latency Networking : Kafka requires low-latency communication. Stretch clusters should be deployed in data centers or availability zones within a single region, avoiding intercontinental deployments. KRaft-based Deployment This stretch cluster implementation requires KRaft as both Strimzi and Kafka are transitioning away from Zookeeper. Cloud native cross-cluster communication technologies Challenges in multi-cluster Kafka deployment Kafka brokers and controllers need stable and low-latency communication across clusters. However, Kubernetes clusters are isolated environments, making cross-cluster networking complex. Some of the key challenges include: Service discovery : Brokers in one cluster need to discover and communicate with brokers in another cluster. Network connectivity : Kubernetes clusters usually have separate network overlays, preventing direct pod-to-pod communication. Security : Ensuring secure cross-cluster communication is critical to prevent unauthorized access. Why Submariner? Submariner is a CNCF project that provides secure cross-cluster networking for Kubernetes. It enables: Direct pod-to-pod connectivity : Allowing Kafka brokers and controllers to communicate seamlessly. Service discovery across Kubernetes cluster boundaries : Kafka services become reachable from any cluster. Secure communication : Using IPsec, WireGuard, or VXLAN for encrypted cross-cluster traffic. Why Cilium? Cilium is an eBPF-based networking solution that enhances Kubernetes security and observability. It provides: Transparent multi-cluster networking : Using Cilium ClusterMesh for service discovery and connectivity. Enhanced security : Enforcing fine-grained security policies across clusters. Performance optimization : Using eBPF for high-performance packet processing. Which one to choose? Both Submariner and Cilium offer cross-cluster networking, but they cater to different use cases: Feature Submariner Cilium Best for Hybrid and multi-cloud deployments Kubernetes-native networking Connectivity Pod-to-pod networking via tunnels Transparent multi-cluster service mesh Security IPsec/WireGuard encryption eBPF-based security policies","title":"Stretch Cluster using Strimzi"},{"location":"#stretch-cluster-using-strimzi","text":"Apache Kafka is widely used for high-throughput, real-time data streaming, but deploying it across multiple Kubernetes clusters presents unique challenges. A Stretch Kafka Cluster is a deployment model where Kafka brokers and controllers are distributed across multiple Kubernetes clusters while maintaining a single logical Kafka cluster. This enables improved fault tolerance, scalability, and disaster recovery while keeping latency low. This documentation provides step-by-step guidance on setting up a Stretch Kafka Cluster using Submariner or Cilium for cross-cluster networking. The goal is to enable seamless communication between Kafka brokers and controllers running in different Kubernetes clusters, ensuring high availability and resilience. Note \ud83d\udea8 This not an offically supported Strimzi feature or documentation.\ud83d\udea8","title":"Stretch Cluster using Strimzi"},{"location":"#overview-of-stretch-kafka-clusters","text":"","title":"Overview of stretch Kafka clusters"},{"location":"#what-is-a-stretch-kafka-cluster","text":"A Stretch Kafka Cluster extends a single Kafka deployment across multiple Kubernetes clusters. Unlike a standard Kafka deployment, which runs within a single Kubernetes cluster, a stretch cluster allows brokers and controllers to be distributed across multiple clusters while functioning as a unified Kafka instance.","title":"What is a stretch Kafka cluster?"},{"location":"#key-benefits-of-stretch-kafka-clusters","text":"High Availability and Disaster Recovery : If one Kubernetes cluster fails, Kafka continues to operate using nodes in the remaining K8s clusters. Scalability : Kafka brokers can be added across multiple K8s clusters to handle increasing workloads. Optimized Workload Distribution : Workloads can be balanced across different K8s clusters for improved resource utilization.","title":"Key benefits of stretch Kafka clusters"},{"location":"#deployment-considerations","text":"Low-Latency Networking : Kafka requires low-latency communication. Stretch clusters should be deployed in data centers or availability zones within a single region, avoiding intercontinental deployments. KRaft-based Deployment This stretch cluster implementation requires KRaft as both Strimzi and Kafka are transitioning away from Zookeeper.","title":"Deployment considerations"},{"location":"#cloud-native-cross-cluster-communication-technologies","text":"","title":"Cloud native cross-cluster communication technologies"},{"location":"#challenges-in-multi-cluster-kafka-deployment","text":"Kafka brokers and controllers need stable and low-latency communication across clusters. However, Kubernetes clusters are isolated environments, making cross-cluster networking complex. Some of the key challenges include: Service discovery : Brokers in one cluster need to discover and communicate with brokers in another cluster. Network connectivity : Kubernetes clusters usually have separate network overlays, preventing direct pod-to-pod communication. Security : Ensuring secure cross-cluster communication is critical to prevent unauthorized access.","title":"Challenges in multi-cluster Kafka deployment"},{"location":"#why-submariner","text":"Submariner is a CNCF project that provides secure cross-cluster networking for Kubernetes. It enables: Direct pod-to-pod connectivity : Allowing Kafka brokers and controllers to communicate seamlessly. Service discovery across Kubernetes cluster boundaries : Kafka services become reachable from any cluster. Secure communication : Using IPsec, WireGuard, or VXLAN for encrypted cross-cluster traffic.","title":"Why Submariner?"},{"location":"#why-cilium","text":"Cilium is an eBPF-based networking solution that enhances Kubernetes security and observability. It provides: Transparent multi-cluster networking : Using Cilium ClusterMesh for service discovery and connectivity. Enhanced security : Enforcing fine-grained security policies across clusters. Performance optimization : Using eBPF for high-performance packet processing.","title":"Why Cilium?"},{"location":"#which-one-to-choose","text":"Both Submariner and Cilium offer cross-cluster networking, but they cater to different use cases: Feature Submariner Cilium Best for Hybrid and multi-cloud deployments Kubernetes-native networking Connectivity Pod-to-pod networking via tunnels Transparent multi-cluster service mesh Security IPsec/WireGuard encryption eBPF-based security policies","title":"Which one to choose?"},{"location":"Deploying-Strimzi-in-Stretch-Mode/","text":"Deploying Strimzi in Stretch Mode This section details how to deploy the Strimzi Kafka Operator in Stretch Mode. The document is divided into two sections: Deploying stretch clusters in OpenShift Container Platform (OCP) Deploying stretch clusters in Kubernetes Let's first look at how to deploy a stretch cluster with OpenShift. Deploying stretch clusters in OpenShift Deploying a stretch cluster in OpenShift is relatively straightforward. Follow these steps: Navigate to the OperatorHub in the OpenShift console. Search for Strimzi and install version 0.44.0 in your namespace. (You must install Strimzi in all Kubernetes clusters that are part of the stretch cluster deployment.) Set Update approval to Manual to prevent OpenShift from automatically updating the operator to the latest stable version. Manually approve the Install Plan and wait a few minutes for the operator to install. (Ensure that namespaces have the same name across all Kubernetes clusters.) Updating KafkaNodePool CRD Once the operator is installed, update the KafkaNodePool CRD by adding the following under .spec.versions[0].schema.openAPIV3Schema.properties.spec.properties . (For prototype testing, update the CRD only in the central cluster.) cluster : type : string description : Target Kubernetes Cluster where SPS will be created. You can edit the CRD using the command: oc edit crd kafkanodepools.kafka.strimzi.io Alternatively, you can update it using the OpenShift console: Navigate to Administration \u2192 CustomResourceDefinitions . Select KafkaNodePool and edit the schema. Updating ClusterRole for Submariner If Submariner is used for pod-to-pod communication, update the strimzi-cluster-operator ClusterRole to allow the creation of the ServiceExport CR (This update is required only in the central cluster). The ServiceExport CR specifies which services should be exported outside the cluster. More details can be found here . The updated ClusterRole should look like this: kind : ClusterRole apiVersion : rbac.authorization.k8s.io/v1 metadata : name : strimzi-cluster-operator.-ty2SyQjgGwxj3ajI1Cg98uIuqXKKouGnWYwjG labels : <REDACTED> rules : - verbs : - get - list - watch - create - delete - patch - update apiGroups : - rbac.authorization.k8s.io - multicluster.x-k8s.io # ----> Add this resources : - clusterrolebindings - serviceexports ## ----> Add this - verbs : - get apiGroups : - storage.k8s.io resources : - storageclasses - verbs : - get - list apiGroups : - '' resources : - nodes Creating Kubernetes secrets for remote cluster access In the central cluster, create Kubernetes secrets to connect to the remote clusters. This is necessary because all CRs ( Kafka and KafkaNodePool ) are created in the central cluster, leading to Kafka pod and related resource creation in the remote clusters. Create files containing Kubeconfig data for each remote cluster (for example): cluster-a-config (for Kubernetes cluster cluster-a) cluster-b-config (for Kubernetes cluster cluster-b) Create secrets in the central cluster: \ud83d\udd25\ud83d\udd25\ud83d\udd25 $ kubectl create secret generic secret-cluster-a \\ --from-file = kubeconfig = cluster-a-config secret/secret-cluster-a created \ud83d\udd25\ud83d\udd25\ud83d\udd25 $ kubectl create secret generic secret-cluster-b \\ --from-file = kubeconfig = cluster-b-config secret/secret-cluster-b created Note \u2705 The secret name does not matter. Use the same secret name in the STRIMZI_K8S_CLUSTERS environment variable. Updating the operator image Navigate to Installed Operators in the OpenShift console. Select Strimzi and open the YAML view. Locate the operator image: quay.io/strimzi/operator@sha256:b07b81f7e282dea2e4e29a7c93cfdd911d4715a5c32fe4c4e3d7c71cba3091e8 4. Replace it with your built image. If using a pre-built image, replace the default 0.44.0 image with: aswinayyolath/stretchcluster:latest Updating environment variables Central cluster - name : STRIMZI_STRETCH_MODE value : 'true' - name : STRIMZI_K8S_CLUSTERS value : | cluster-a.url=<cluster-a URL> cluster-a.secret=secret-cluster-a cluster-b.url=<cluster-b URL> cluster-b.secret=secret-cluster-b - name : STRIMZI_NETWORK_POLICY_GENERATION value : 'false' Remote clusters - name : STRIMZI_STRETCH_MODE value : 'true' Note \u2705 Optionally, we can set STRIMZI_POD_SET_RECONCILIATION_ONLY to true in the remote cluster operator deployment. This ensures that only the SPS controller runs, preventing other controllers from starting and handling other custom resources. Logs confirming that no other operators have started except the SPS controller: 2025 -03-17 08 :54:49 INFO PodSecurityProviderFactory:43 - Found PodSecurityProvider io.strimzi.plugin.security.profiles.impl.BaselinePodSecurityProvider 2025 -03-17 08 :54:49 INFO PodSecurityProviderFactory:62 - Initializing PodSecurityProvider io.strimzi.plugin.security.profiles.impl.BaselinePodSecurityProvider 2025 -03-17 08 :54:49 INFO ClusterOperator:86 - Creating ClusterOperator for namespace strimzi 2025 -03-17 08 :54:49 INFO ClusterOperator:100 - Starting ClusterOperator for namespace strimzi 2025 -03-17 08 :54:49 INFO ClusterOperator:154 - --Stretch Mode value in ClusterOperator-- true 2025 -03-17 08 :54:49 INFO StrimziPodSetController:594 - Starting the StrimziPodSet controller 2025 -03-17 08 :54:49 INFO ClusterOperator:137 - Setting up periodic reconciliation for namespace strimzi 2025 -03-17 08 :54:49 INFO StrimziPodSetController:563 - Starting StrimziPodSet controller for namespace strimzi 2025 -03-17 08 :54:49 INFO Main:193 - Cluster Operator verticle started in namespace strimzi without label selector 2025 -03-17 08 :54:49 WARN VersionUsageUtils:60 - The client is using resource type 'strimzipodsets' with unstable version 'v1beta2' 2025 -03-17 08 :54:49 WARN VersionUsageUtils:60 - The client is using resource type 'kafkas' with unstable version 'v1beta2' 2025 -03-17 08 :54:49 WARN VersionUsageUtils:60 - The client is using resource type 'kafkaconnects' with unstable version 'v1beta2' 2025 -03-17 08 :54:49 WARN VersionUsageUtils:60 - The client is using resource type 'kafkamirrormaker2s' with unstable version 'v1beta2' 2025 -03-17 08 :54:49 INFO StrimziPodSetController:566 - Waiting for informers to sync 2025 -03-17 08 :54:52 INFO StrimziPodSetController:571 - Informers are in -sync 2025 -03-17 08 :54:53 INFO StrimziPodSetController:389 - Reconciliation #1(watch) StrimziPodSet(strimzi/my-cluster-stretch2-broker): StrimziPodSet will be reconciled 2025 -03-17 08 :54:53 INFO StrimziPodSetController:425 - Reconciliation #1(watch) StrimziPodSet(strimzi/my-cluster-stretch2-broker): reconciled 2025 -03-17 08 :54:53 INFO StrimziPodSetController:389 - Reconciliation #2(watch) StrimziPodSet(strimzi/my-cluster-stretch2-controller): StrimziPodSet will be reconciled 2025 -03-17 08 :54:53 INFO StrimziPodSetController:425 - Reconciliation #2(watch) StrimziPodSet(strimzi/my-cluster-stretch2-controller): reconciled 2025 -03-17 08 :54:53 INFO StrimziPodSetController:389 - Reconciliation #3(watch) StrimziPodSet(strimzi/my-cluster-stretch2-broker): StrimziPodSet will be reconciled 2025 -03-17 08 :54:53 INFO StrimziPodSetController:425 - Reconciliation #3(watch) StrimziPodSet(strimzi/my-cluster-stretch2-broker): reconciled 2025 -03-17 08 :59:49 INFO StrimziPodSetController:389 - Reconciliation #4(watch) StrimziPodSet(strimzi/my-cluster-stretch2-broker): StrimziPodSet will be reconciled After making these changes, save the ClusterServiceVersion YAML, which will trigger a restart of the operator pod. Applying Kafka and KafkaNodePool CRs in the central cluster Once the setup is complete, apply the Kafka and KafkaNodePool CRs in the central cluster. Below are example CRs: #Central Cluster CR (spec.cluster is missing in these CRs) #---------------------------------------------------------- apiVersion : kafka.strimzi.io/v1beta2 kind : KafkaNodePool metadata : name : controller labels : strimzi.io/cluster : my-cluster strimzi.io/submariner-cluster-id : \"cluster1\" #-------> submariner-cluster-id will be used in controller.quorum.voters and advertised.listeners to enable cross cluster communication spec : #-------> In addition to this every controller and broker Pods will have a SANS entry with the Submariner exported DNS name replicas : 3 roles : - controller storage : type : jbod volumes : - id : 0 type : ephemeral kraftMetadata : shared --- apiVersion : kafka.strimzi.io/v1beta2 kind : KafkaNodePool metadata : name : broker labels : strimzi.io/cluster : my-cluster strimzi.io/submariner-cluster-id : \"cluster1\" spec : replicas : 3 roles : - broker storage : type : jbod volumes : - id : 0 type : ephemeral kraftMetadata : shared --- #Remote Cluster CR (spec.cluster is present and the SPS will be deployed on K8s cluster which we referenced as cluster-a) #----------------------------------------------------------------------------------------------------------------------- apiVersion : kafka.strimzi.io/v1beta2 kind : KafkaNodePool metadata : name : stretch1-controller labels : strimzi.io/cluster : my-cluster strimzi.io/submariner-cluster-id : \"cluster2\" spec : cluster : cluster-a #-------> all resources part of this KNP will be created in K8s cluster-a replicas : 3 roles : - controller storage : type : jbod volumes : - id : 0 type : ephemeral kraftMetadata : shared --- apiVersion : kafka.strimzi.io/v1beta2 kind : KafkaNodePool metadata : name : stretch1-broker labels : strimzi.io/cluster : my-cluster strimzi.io/submariner-cluster-id : \"cluster2\" spec : cluster : cluster-a #-------> all resources part of this KNP will be created in K8s cluster-a replicas : 3 roles : - broker storage : type : jbod volumes : - id : 0 type : ephemeral kraftMetadata : shared --- #Remote Cluster CR (spec.cluster is present and the SPS will be deployed on K8s cluster which we referenced as cluster-b) #----------------------------------------------------------------------------------------------------------------------- apiVersion : kafka.strimzi.io/v1beta2 kind : KafkaNodePool metadata : name : stretch2-controller labels : strimzi.io/cluster : my-cluster strimzi.io/submariner-cluster-id : \"cluster3\" spec : cluster : cluster-b #-------> all resources part of this KNP will be created in K8s cluster-b replicas : 3 roles : - controller storage : type : jbod volumes : - id : 0 type : ephemeral kraftMetadata : shared --- apiVersion : kafka.strimzi.io/v1beta2 kind : KafkaNodePool metadata : name : stretch2-broker labels : strimzi.io/cluster : my-cluster strimzi.io/submariner-cluster-id : \"cluster3\" spec : cluster : cluster-b #-------> all resources part of this KNP will be created in K8s cluster-b replicas : 3 roles : - broker storage : type : jbod volumes : - id : 0 type : ephemeral kraftMetadata : shared --- apiVersion : kafka.strimzi.io/v1beta2 kind : Kafka metadata : name : my-cluster annotations : strimzi.io/node-pools : enabled strimzi.io/kraft : enabled strimzi.io/cross-cluster-type : \"submariner\" #-------> Cross Cluster Communication Technology in use spec : kafka : version : 3.8.0 metadataVersion : 3.8-IV0 listeners : - name : plain port : 9092 type : internal tls : false - name : tls port : 9093 type : internal tls : true config : offsets.topic.replication.factor : 3 transaction.state.log.replication.factor : 3 transaction.state.log.min.isr : 2 default.replication.factor : 3 min.insync.replicas : 2 entityOperator : topicOperator : {} userOperator : {} How it works spec.cluster in KafkaNodePool (KNP) The spec.cluster field in a KafkaNodePool (KNP) CR determines where the KafkaNodePool will be created: If spec.cluster is missing, the KafkaNodePool is assumed to be created in the central Kubernetes cluster (i.e., the cluster where the Kafka CR is applied). If spec.cluster is defined, the provided cluster name will be matched against the STRIMZI_K8S_CLUSTERS environment variable. The corresponding secrets for remote Kubernetes clusters will then be used to create the required resources in the specified cluster. Submariner cluster ID A Submariner cluster ID can be defined using the submariner-cluster-id label in all KafkaNodePool CRs. This label represents the cluster identifier used by Submariner for tunnel identification. Each cluster must have a unique Submariner cluster ID to ensure seamless communication. Cross-cluster type Instead of defining the cross-cluster type separately for each KafkaNodePool , it can be centrally managed using the cross-cluster-type annotation in the Kafka CR. The Kafka CR serves as a logical place to store this information since the cross-cluster type is a shared property across all clusters in a stretch Kafka setup. This ensures consistency and simplicity in configuration by avoiding redundant definitions in multiple KafkaNodePool CRs. STRIMZI_NETWORK_POLICY_GENERATION You need to set STRIMZI_NETWORK_POLICY_GENERATION to false because the default NetworkPolicy created by Strimzi restricts traffic to Kafka pods within a specific namespace. By default, Kafka pods can only receive traffic from: Kafka clients and Kafka-related components within the same cluster (on port 9090). Specific Strimzi components such as: Cluster operator Entity operator Kafka exporter Cruise control (on ports 9091, 8443, 9092, and 9093). This policy improves security by ensuring that only necessary services can communicate with Kafka pods. However, it also blocks traffic between Kubernetes clusters, which is required for stretch cluster deployments. To allow communication between Kubernetes clusters, you must set STRIMZI_NETWORK_POLICY_GENERATION to false . STRIMZI_STRETCH_MODE By default, Strimzi expects both the Kafka and KafkaNodePool (KNP) resources to be present in the same cluster where it creates the StrimziPodSet (SPS) and Kafka pods. However, in a stretch cluster deployment, the member clusters (remote clusters) do not contain the Kafka and KafkaNodePool CRs. To address this, we introduced the STRIMZI_STRETCH_MODE environment variable. When STRIMZI_STRETCH_MODE is set to true , the Strimzi operator bypasses validation checks that require Kafka and KafkaNodePool CRs to exist in the cluster where Kafka pods are deployed. This allows Strimzi to create Kafka pods in remote clusters without requiring Kafka or KafkaNodePool CRs in those clusters. Summary Use the submariner-cluster-id label in KafkaNodePool CRs to uniquely identify each cluster in Submariner\u2019s tunnel setup. Use the cross-cluster-type annotation in the Kafka CR to centrally define the cross-cluster communication type. Leverage spec.cluster in KafkaNodePool to determine the target cluster for resource creation, defaulting to the central cluster if not specified. Central cluster considerations The current setup functions even if no KafkaNodePool (KNP) resources are created for the central cluster. This means: KafkaNodePool CRs can be applied in the central cluster, triggering the creation of pods and Kubernetes resources only in remote clusters, with no additional resources required in the central cluster itself. In other words, it is entirely possible to create a stretch Kafka cluster where the central cluster hosts only the Strimzi cluster operator, with Kafka brokers running exclusively in remote clusters. This eliminates any dependency on having a KafkaNodePool resource in the central cluster. Deploying stretch clusters in Kubernetes Deploying a stretch cluster in Kubernetes follows a similar process as in OpenShift, with a few key differences: Installing Strimzi Editing the cluster operator Deployment instead of the ClusterServiceVersion Installing Strimzi Download Strimzi 0.44.0 using the following command: wget https://github.com/strimzi/strimzi-kafka-operator/releases/download/0.44.0/strimzi-0.44.0.tar.gz Alternatively, you can download Strimzi 0.44.0 from the release page . Extract the downloaded file tar -xvzf strimzi-0.44.0.tar.gz By default, Strimzi resources are configured to work in the myproject namespace. If you want to use a different namespace, update the namespace references in the relevant files using: sed -i 's/namespace: .*/namespace: <yournamespace>/' install/cluster-operator/*RoleBinding*.yaml Edit the install/cluster-operator/060-Deployment-strimzi-cluster-operator.yaml file: Set the STRIMZI_NAMESPACE environment variable to yournamespace . Add the following environment variables: Central cluster configuration Modify 060-Deployment-strimzi-cluster-operator.yaml and add: - name : STRIMZI_STRETCH_MODE value : 'true' - name : STRIMZI_K8S_CLUSTERS value : | cluster-a.url=<cluster-a URL> cluster-a.secret=secret-cluster-a cluster-b.url=<cluster-b URL> cluster-b.secret=secret-cluster-b - name : STRIMZI_NETWORK_POLICY_GENERATION value : 'false' Remote cluster configuration For remote clusters, add only: - name : STRIMZI_STRETCH_MODE value : 'true' Granting permissions to the cluster operator kubectl create -f install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml -n <yournamespace> kubectl create -f install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml -n <yournamespace> These commands create role bindings that grant the cluster operator permission to access the Kafka cluster. Deploy CRDs and RBAC resources Deploy the Custom Resource Definitions (CRDs) and role-based access control (RBAC) resources: kubectl create -f install/cluster-operator/ -n <yournamespace> Remaining steps The remaining steps are the same for both OpenShift and Kubernetes, such as: Updating the KafkaNodePool CRD Updating the strimzi-cluster-operator ClusterRole Creating a Kubeconfig secret in the central cluster","title":"Deploying Strimzi in Stretch Mode"},{"location":"Deploying-Strimzi-in-Stretch-Mode/#deploying-strimzi-in-stretch-mode","text":"This section details how to deploy the Strimzi Kafka Operator in Stretch Mode. The document is divided into two sections: Deploying stretch clusters in OpenShift Container Platform (OCP) Deploying stretch clusters in Kubernetes Let's first look at how to deploy a stretch cluster with OpenShift.","title":"Deploying Strimzi in Stretch Mode"},{"location":"Deploying-Strimzi-in-Stretch-Mode/#deploying-stretch-clusters-in-openshift","text":"Deploying a stretch cluster in OpenShift is relatively straightforward. Follow these steps: Navigate to the OperatorHub in the OpenShift console. Search for Strimzi and install version 0.44.0 in your namespace. (You must install Strimzi in all Kubernetes clusters that are part of the stretch cluster deployment.) Set Update approval to Manual to prevent OpenShift from automatically updating the operator to the latest stable version. Manually approve the Install Plan and wait a few minutes for the operator to install. (Ensure that namespaces have the same name across all Kubernetes clusters.)","title":"Deploying stretch clusters in OpenShift"},{"location":"Deploying-Strimzi-in-Stretch-Mode/#updating-kafkanodepool-crd","text":"Once the operator is installed, update the KafkaNodePool CRD by adding the following under .spec.versions[0].schema.openAPIV3Schema.properties.spec.properties . (For prototype testing, update the CRD only in the central cluster.) cluster : type : string description : Target Kubernetes Cluster where SPS will be created. You can edit the CRD using the command: oc edit crd kafkanodepools.kafka.strimzi.io Alternatively, you can update it using the OpenShift console: Navigate to Administration \u2192 CustomResourceDefinitions . Select KafkaNodePool and edit the schema.","title":"Updating KafkaNodePool CRD"},{"location":"Deploying-Strimzi-in-Stretch-Mode/#updating-clusterrole-for-submariner","text":"If Submariner is used for pod-to-pod communication, update the strimzi-cluster-operator ClusterRole to allow the creation of the ServiceExport CR (This update is required only in the central cluster). The ServiceExport CR specifies which services should be exported outside the cluster. More details can be found here . The updated ClusterRole should look like this: kind : ClusterRole apiVersion : rbac.authorization.k8s.io/v1 metadata : name : strimzi-cluster-operator.-ty2SyQjgGwxj3ajI1Cg98uIuqXKKouGnWYwjG labels : <REDACTED> rules : - verbs : - get - list - watch - create - delete - patch - update apiGroups : - rbac.authorization.k8s.io - multicluster.x-k8s.io # ----> Add this resources : - clusterrolebindings - serviceexports ## ----> Add this - verbs : - get apiGroups : - storage.k8s.io resources : - storageclasses - verbs : - get - list apiGroups : - '' resources : - nodes","title":"Updating ClusterRole for Submariner"},{"location":"Deploying-Strimzi-in-Stretch-Mode/#creating-kubernetes-secrets-for-remote-cluster-access","text":"In the central cluster, create Kubernetes secrets to connect to the remote clusters. This is necessary because all CRs ( Kafka and KafkaNodePool ) are created in the central cluster, leading to Kafka pod and related resource creation in the remote clusters. Create files containing Kubeconfig data for each remote cluster (for example): cluster-a-config (for Kubernetes cluster cluster-a) cluster-b-config (for Kubernetes cluster cluster-b) Create secrets in the central cluster: \ud83d\udd25\ud83d\udd25\ud83d\udd25 $ kubectl create secret generic secret-cluster-a \\ --from-file = kubeconfig = cluster-a-config secret/secret-cluster-a created \ud83d\udd25\ud83d\udd25\ud83d\udd25 $ kubectl create secret generic secret-cluster-b \\ --from-file = kubeconfig = cluster-b-config secret/secret-cluster-b created Note \u2705 The secret name does not matter. Use the same secret name in the STRIMZI_K8S_CLUSTERS environment variable.","title":"Creating Kubernetes secrets for remote cluster access"},{"location":"Deploying-Strimzi-in-Stretch-Mode/#updating-the-operator-image","text":"Navigate to Installed Operators in the OpenShift console. Select Strimzi and open the YAML view. Locate the operator image: quay.io/strimzi/operator@sha256:b07b81f7e282dea2e4e29a7c93cfdd911d4715a5c32fe4c4e3d7c71cba3091e8 4. Replace it with your built image. If using a pre-built image, replace the default 0.44.0 image with: aswinayyolath/stretchcluster:latest","title":"Updating the operator image"},{"location":"Deploying-Strimzi-in-Stretch-Mode/#updating-environment-variables","text":"Central cluster - name : STRIMZI_STRETCH_MODE value : 'true' - name : STRIMZI_K8S_CLUSTERS value : | cluster-a.url=<cluster-a URL> cluster-a.secret=secret-cluster-a cluster-b.url=<cluster-b URL> cluster-b.secret=secret-cluster-b - name : STRIMZI_NETWORK_POLICY_GENERATION value : 'false' Remote clusters - name : STRIMZI_STRETCH_MODE value : 'true' Note \u2705 Optionally, we can set STRIMZI_POD_SET_RECONCILIATION_ONLY to true in the remote cluster operator deployment. This ensures that only the SPS controller runs, preventing other controllers from starting and handling other custom resources. Logs confirming that no other operators have started except the SPS controller: 2025 -03-17 08 :54:49 INFO PodSecurityProviderFactory:43 - Found PodSecurityProvider io.strimzi.plugin.security.profiles.impl.BaselinePodSecurityProvider 2025 -03-17 08 :54:49 INFO PodSecurityProviderFactory:62 - Initializing PodSecurityProvider io.strimzi.plugin.security.profiles.impl.BaselinePodSecurityProvider 2025 -03-17 08 :54:49 INFO ClusterOperator:86 - Creating ClusterOperator for namespace strimzi 2025 -03-17 08 :54:49 INFO ClusterOperator:100 - Starting ClusterOperator for namespace strimzi 2025 -03-17 08 :54:49 INFO ClusterOperator:154 - --Stretch Mode value in ClusterOperator-- true 2025 -03-17 08 :54:49 INFO StrimziPodSetController:594 - Starting the StrimziPodSet controller 2025 -03-17 08 :54:49 INFO ClusterOperator:137 - Setting up periodic reconciliation for namespace strimzi 2025 -03-17 08 :54:49 INFO StrimziPodSetController:563 - Starting StrimziPodSet controller for namespace strimzi 2025 -03-17 08 :54:49 INFO Main:193 - Cluster Operator verticle started in namespace strimzi without label selector 2025 -03-17 08 :54:49 WARN VersionUsageUtils:60 - The client is using resource type 'strimzipodsets' with unstable version 'v1beta2' 2025 -03-17 08 :54:49 WARN VersionUsageUtils:60 - The client is using resource type 'kafkas' with unstable version 'v1beta2' 2025 -03-17 08 :54:49 WARN VersionUsageUtils:60 - The client is using resource type 'kafkaconnects' with unstable version 'v1beta2' 2025 -03-17 08 :54:49 WARN VersionUsageUtils:60 - The client is using resource type 'kafkamirrormaker2s' with unstable version 'v1beta2' 2025 -03-17 08 :54:49 INFO StrimziPodSetController:566 - Waiting for informers to sync 2025 -03-17 08 :54:52 INFO StrimziPodSetController:571 - Informers are in -sync 2025 -03-17 08 :54:53 INFO StrimziPodSetController:389 - Reconciliation #1(watch) StrimziPodSet(strimzi/my-cluster-stretch2-broker): StrimziPodSet will be reconciled 2025 -03-17 08 :54:53 INFO StrimziPodSetController:425 - Reconciliation #1(watch) StrimziPodSet(strimzi/my-cluster-stretch2-broker): reconciled 2025 -03-17 08 :54:53 INFO StrimziPodSetController:389 - Reconciliation #2(watch) StrimziPodSet(strimzi/my-cluster-stretch2-controller): StrimziPodSet will be reconciled 2025 -03-17 08 :54:53 INFO StrimziPodSetController:425 - Reconciliation #2(watch) StrimziPodSet(strimzi/my-cluster-stretch2-controller): reconciled 2025 -03-17 08 :54:53 INFO StrimziPodSetController:389 - Reconciliation #3(watch) StrimziPodSet(strimzi/my-cluster-stretch2-broker): StrimziPodSet will be reconciled 2025 -03-17 08 :54:53 INFO StrimziPodSetController:425 - Reconciliation #3(watch) StrimziPodSet(strimzi/my-cluster-stretch2-broker): reconciled 2025 -03-17 08 :59:49 INFO StrimziPodSetController:389 - Reconciliation #4(watch) StrimziPodSet(strimzi/my-cluster-stretch2-broker): StrimziPodSet will be reconciled After making these changes, save the ClusterServiceVersion YAML, which will trigger a restart of the operator pod.","title":"Updating environment variables"},{"location":"Deploying-Strimzi-in-Stretch-Mode/#applying-kafka-and-kafkanodepool-crs-in-the-central-cluster","text":"Once the setup is complete, apply the Kafka and KafkaNodePool CRs in the central cluster. Below are example CRs: #Central Cluster CR (spec.cluster is missing in these CRs) #---------------------------------------------------------- apiVersion : kafka.strimzi.io/v1beta2 kind : KafkaNodePool metadata : name : controller labels : strimzi.io/cluster : my-cluster strimzi.io/submariner-cluster-id : \"cluster1\" #-------> submariner-cluster-id will be used in controller.quorum.voters and advertised.listeners to enable cross cluster communication spec : #-------> In addition to this every controller and broker Pods will have a SANS entry with the Submariner exported DNS name replicas : 3 roles : - controller storage : type : jbod volumes : - id : 0 type : ephemeral kraftMetadata : shared --- apiVersion : kafka.strimzi.io/v1beta2 kind : KafkaNodePool metadata : name : broker labels : strimzi.io/cluster : my-cluster strimzi.io/submariner-cluster-id : \"cluster1\" spec : replicas : 3 roles : - broker storage : type : jbod volumes : - id : 0 type : ephemeral kraftMetadata : shared --- #Remote Cluster CR (spec.cluster is present and the SPS will be deployed on K8s cluster which we referenced as cluster-a) #----------------------------------------------------------------------------------------------------------------------- apiVersion : kafka.strimzi.io/v1beta2 kind : KafkaNodePool metadata : name : stretch1-controller labels : strimzi.io/cluster : my-cluster strimzi.io/submariner-cluster-id : \"cluster2\" spec : cluster : cluster-a #-------> all resources part of this KNP will be created in K8s cluster-a replicas : 3 roles : - controller storage : type : jbod volumes : - id : 0 type : ephemeral kraftMetadata : shared --- apiVersion : kafka.strimzi.io/v1beta2 kind : KafkaNodePool metadata : name : stretch1-broker labels : strimzi.io/cluster : my-cluster strimzi.io/submariner-cluster-id : \"cluster2\" spec : cluster : cluster-a #-------> all resources part of this KNP will be created in K8s cluster-a replicas : 3 roles : - broker storage : type : jbod volumes : - id : 0 type : ephemeral kraftMetadata : shared --- #Remote Cluster CR (spec.cluster is present and the SPS will be deployed on K8s cluster which we referenced as cluster-b) #----------------------------------------------------------------------------------------------------------------------- apiVersion : kafka.strimzi.io/v1beta2 kind : KafkaNodePool metadata : name : stretch2-controller labels : strimzi.io/cluster : my-cluster strimzi.io/submariner-cluster-id : \"cluster3\" spec : cluster : cluster-b #-------> all resources part of this KNP will be created in K8s cluster-b replicas : 3 roles : - controller storage : type : jbod volumes : - id : 0 type : ephemeral kraftMetadata : shared --- apiVersion : kafka.strimzi.io/v1beta2 kind : KafkaNodePool metadata : name : stretch2-broker labels : strimzi.io/cluster : my-cluster strimzi.io/submariner-cluster-id : \"cluster3\" spec : cluster : cluster-b #-------> all resources part of this KNP will be created in K8s cluster-b replicas : 3 roles : - broker storage : type : jbod volumes : - id : 0 type : ephemeral kraftMetadata : shared --- apiVersion : kafka.strimzi.io/v1beta2 kind : Kafka metadata : name : my-cluster annotations : strimzi.io/node-pools : enabled strimzi.io/kraft : enabled strimzi.io/cross-cluster-type : \"submariner\" #-------> Cross Cluster Communication Technology in use spec : kafka : version : 3.8.0 metadataVersion : 3.8-IV0 listeners : - name : plain port : 9092 type : internal tls : false - name : tls port : 9093 type : internal tls : true config : offsets.topic.replication.factor : 3 transaction.state.log.replication.factor : 3 transaction.state.log.min.isr : 2 default.replication.factor : 3 min.insync.replicas : 2 entityOperator : topicOperator : {} userOperator : {}","title":"Applying Kafka and KafkaNodePool CRs in the central cluster"},{"location":"Deploying-Strimzi-in-Stretch-Mode/#how-it-works","text":"","title":"How it works"},{"location":"Deploying-Strimzi-in-Stretch-Mode/#speccluster-in-kafkanodepool-knp","text":"The spec.cluster field in a KafkaNodePool (KNP) CR determines where the KafkaNodePool will be created: If spec.cluster is missing, the KafkaNodePool is assumed to be created in the central Kubernetes cluster (i.e., the cluster where the Kafka CR is applied). If spec.cluster is defined, the provided cluster name will be matched against the STRIMZI_K8S_CLUSTERS environment variable. The corresponding secrets for remote Kubernetes clusters will then be used to create the required resources in the specified cluster.","title":"spec.cluster in KafkaNodePool (KNP)"},{"location":"Deploying-Strimzi-in-Stretch-Mode/#submariner-cluster-id","text":"A Submariner cluster ID can be defined using the submariner-cluster-id label in all KafkaNodePool CRs. This label represents the cluster identifier used by Submariner for tunnel identification. Each cluster must have a unique Submariner cluster ID to ensure seamless communication.","title":"Submariner cluster ID"},{"location":"Deploying-Strimzi-in-Stretch-Mode/#cross-cluster-type","text":"Instead of defining the cross-cluster type separately for each KafkaNodePool , it can be centrally managed using the cross-cluster-type annotation in the Kafka CR. The Kafka CR serves as a logical place to store this information since the cross-cluster type is a shared property across all clusters in a stretch Kafka setup. This ensures consistency and simplicity in configuration by avoiding redundant definitions in multiple KafkaNodePool CRs.","title":"Cross-cluster type"},{"location":"Deploying-Strimzi-in-Stretch-Mode/#strimzi_network_policy_generation","text":"You need to set STRIMZI_NETWORK_POLICY_GENERATION to false because the default NetworkPolicy created by Strimzi restricts traffic to Kafka pods within a specific namespace. By default, Kafka pods can only receive traffic from: Kafka clients and Kafka-related components within the same cluster (on port 9090). Specific Strimzi components such as: Cluster operator Entity operator Kafka exporter Cruise control (on ports 9091, 8443, 9092, and 9093). This policy improves security by ensuring that only necessary services can communicate with Kafka pods. However, it also blocks traffic between Kubernetes clusters, which is required for stretch cluster deployments. To allow communication between Kubernetes clusters, you must set STRIMZI_NETWORK_POLICY_GENERATION to false .","title":"STRIMZI_NETWORK_POLICY_GENERATION"},{"location":"Deploying-Strimzi-in-Stretch-Mode/#strimzi_stretch_mode","text":"By default, Strimzi expects both the Kafka and KafkaNodePool (KNP) resources to be present in the same cluster where it creates the StrimziPodSet (SPS) and Kafka pods. However, in a stretch cluster deployment, the member clusters (remote clusters) do not contain the Kafka and KafkaNodePool CRs. To address this, we introduced the STRIMZI_STRETCH_MODE environment variable. When STRIMZI_STRETCH_MODE is set to true , the Strimzi operator bypasses validation checks that require Kafka and KafkaNodePool CRs to exist in the cluster where Kafka pods are deployed. This allows Strimzi to create Kafka pods in remote clusters without requiring Kafka or KafkaNodePool CRs in those clusters.","title":"STRIMZI_STRETCH_MODE"},{"location":"Deploying-Strimzi-in-Stretch-Mode/#summary","text":"Use the submariner-cluster-id label in KafkaNodePool CRs to uniquely identify each cluster in Submariner\u2019s tunnel setup. Use the cross-cluster-type annotation in the Kafka CR to centrally define the cross-cluster communication type. Leverage spec.cluster in KafkaNodePool to determine the target cluster for resource creation, defaulting to the central cluster if not specified.","title":"Summary"},{"location":"Deploying-Strimzi-in-Stretch-Mode/#central-cluster-considerations","text":"The current setup functions even if no KafkaNodePool (KNP) resources are created for the central cluster. This means: KafkaNodePool CRs can be applied in the central cluster, triggering the creation of pods and Kubernetes resources only in remote clusters, with no additional resources required in the central cluster itself. In other words, it is entirely possible to create a stretch Kafka cluster where the central cluster hosts only the Strimzi cluster operator, with Kafka brokers running exclusively in remote clusters. This eliminates any dependency on having a KafkaNodePool resource in the central cluster.","title":"Central cluster considerations"},{"location":"Deploying-Strimzi-in-Stretch-Mode/#deploying-stretch-clusters-in-kubernetes","text":"Deploying a stretch cluster in Kubernetes follows a similar process as in OpenShift, with a few key differences: Installing Strimzi Editing the cluster operator Deployment instead of the ClusterServiceVersion","title":"Deploying stretch clusters in Kubernetes"},{"location":"Deploying-Strimzi-in-Stretch-Mode/#installing-strimzi","text":"Download Strimzi 0.44.0 using the following command: wget https://github.com/strimzi/strimzi-kafka-operator/releases/download/0.44.0/strimzi-0.44.0.tar.gz Alternatively, you can download Strimzi 0.44.0 from the release page .","title":"Installing Strimzi"},{"location":"Deploying-Strimzi-in-Stretch-Mode/#extract-the-downloaded-file","text":"tar -xvzf strimzi-0.44.0.tar.gz By default, Strimzi resources are configured to work in the myproject namespace. If you want to use a different namespace, update the namespace references in the relevant files using: sed -i 's/namespace: .*/namespace: <yournamespace>/' install/cluster-operator/*RoleBinding*.yaml Edit the install/cluster-operator/060-Deployment-strimzi-cluster-operator.yaml file: Set the STRIMZI_NAMESPACE environment variable to yournamespace . Add the following environment variables: Central cluster configuration Modify 060-Deployment-strimzi-cluster-operator.yaml and add: - name : STRIMZI_STRETCH_MODE value : 'true' - name : STRIMZI_K8S_CLUSTERS value : | cluster-a.url=<cluster-a URL> cluster-a.secret=secret-cluster-a cluster-b.url=<cluster-b URL> cluster-b.secret=secret-cluster-b - name : STRIMZI_NETWORK_POLICY_GENERATION value : 'false' Remote cluster configuration For remote clusters, add only: - name : STRIMZI_STRETCH_MODE value : 'true'","title":"Extract the downloaded file"},{"location":"Deploying-Strimzi-in-Stretch-Mode/#granting-permissions-to-the-cluster-operator","text":"kubectl create -f install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml -n <yournamespace> kubectl create -f install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml -n <yournamespace> These commands create role bindings that grant the cluster operator permission to access the Kafka cluster.","title":"Granting permissions to the cluster operator"},{"location":"Deploying-Strimzi-in-Stretch-Mode/#deploy-crds-and-rbac-resources","text":"Deploy the Custom Resource Definitions (CRDs) and role-based access control (RBAC) resources: kubectl create -f install/cluster-operator/ -n <yournamespace>","title":"Deploy CRDs and RBAC resources"},{"location":"Deploying-Strimzi-in-Stretch-Mode/#remaining-steps","text":"The remaining steps are the same for both OpenShift and Kubernetes, such as: Updating the KafkaNodePool CRD Updating the strimzi-cluster-operator ClusterRole Creating a Kubeconfig secret in the central cluster","title":"Remaining steps"},{"location":"Installing-Submariner-on-Kind/","text":"Installing Submariner on Kind This section details installing Submariner on Kind using Calico CNI. Installing Kind with Calico CNI We use Submariner's Shipyard project to install Submariner on Kind: $ git clone https://github.com/submariner-io/shipyard.git $ cd shipyard/ $ cat > deploy.two.clusters.nocni.yaml << EOF nodes: control-plane worker clusters: cluster1: cni: none cluster2: cni: none EOF $ make SETTINGS = deploy.two.clusters.nocni.yaml clusters Increasing inotify resource limits Kind clusters have default resource limits that may be insufficient for stretch Kafka clusters. Increase the limits using: $ sudo sysctl fs.inotify.max_user_watches = 524288 $ sudo sysctl fs.inotify.max_user_instances = 512 Verifying cluster installation List clusters $ kind get clusters Check current context Set the correct KUBECONFIG environment variable and list available contexts: $ export KUBECONFIG = $( find $( git rev-parse --show-toplevel ) /output/kubeconfigs/ -type f -printf %p: ) $ kubectl config get-contexts Verify node counts in each cluster $ kubectl --context cluster1 get nodes $ kubectl --context cluster2 get nodes Deploying Calico Create a folder for Calico manifests: $ mkdir calico_manifests Install Calico on cluster 1 $ kubectl --context cluster1 create -f https://raw.githubusercontent.com/projectcalico/calico/v3.29.0/manifests/tigera-operator.yaml $ wget -O calico_manifests/custom-resources.yaml https://raw.githubusercontent.com/projectcalico/calico/v3.29.0/manifests/custom-resources.yaml $ sed -i 's,cidr: 192.168.0.0/16,cidr: 10.130.0.0/16,g' calico_manifests/custom-resources.yaml $ sed -i 's,VXLANCrossSubnet,VXLAN,g' calico_manifests/custom-resources.yaml $ kubectl --context cluster1 apply -f calico_manifests/custom-resources.yaml Install Calico on cluster 2 $ kubectl --context cluster2 create -f https://raw.githubusercontent.com/projectcalico/calico/v3.29.0/manifests/tigera-operator.yaml $ wget -O calico_manifests/custom-resources.yaml https://raw.githubusercontent.com/projectcalico/calico/v3.29.0/manifests/custom-resources.yaml $ sed -i 's,cidr: 192.168.0.0/16,cidr: 10.131.0.0/16,g' calico_manifests/custom-resources.yaml $ sed -i 's,VXLANCrossSubnet,VXLAN,g' calico_manifests/custom-resources.yaml $ kubectl --context cluster2 apply -f calico_manifests/custom-resources.yaml Deploying Submariner Deploying the broker $ subctl deploy-broker --context cluster1 Connecting clusters to the broker $ subctl join --context cluster1 broker-info.subm --clusterid cluster1 --natt = false $ subctl join --context cluster2 broker-info.subm --clusterid cluster2 --natt = false Checking Submariner connections $ subctl show connections --context cluster2 $ subctl show connections --context cluster1","title":"Installing Submariner on Kind"},{"location":"Installing-Submariner-on-Kind/#installing-submariner-on-kind","text":"This section details installing Submariner on Kind using Calico CNI.","title":"Installing Submariner on Kind"},{"location":"Installing-Submariner-on-Kind/#installing-kind-with-calico-cni","text":"We use Submariner's Shipyard project to install Submariner on Kind: $ git clone https://github.com/submariner-io/shipyard.git $ cd shipyard/ $ cat > deploy.two.clusters.nocni.yaml << EOF nodes: control-plane worker clusters: cluster1: cni: none cluster2: cni: none EOF $ make SETTINGS = deploy.two.clusters.nocni.yaml clusters","title":"Installing Kind with Calico CNI"},{"location":"Installing-Submariner-on-Kind/#increasing-inotify-resource-limits","text":"Kind clusters have default resource limits that may be insufficient for stretch Kafka clusters. Increase the limits using: $ sudo sysctl fs.inotify.max_user_watches = 524288 $ sudo sysctl fs.inotify.max_user_instances = 512","title":"Increasing inotify resource limits"},{"location":"Installing-Submariner-on-Kind/#verifying-cluster-installation","text":"","title":"Verifying cluster installation"},{"location":"Installing-Submariner-on-Kind/#list-clusters","text":"$ kind get clusters","title":"List clusters"},{"location":"Installing-Submariner-on-Kind/#check-current-context","text":"Set the correct KUBECONFIG environment variable and list available contexts: $ export KUBECONFIG = $( find $( git rev-parse --show-toplevel ) /output/kubeconfigs/ -type f -printf %p: ) $ kubectl config get-contexts","title":"Check current context"},{"location":"Installing-Submariner-on-Kind/#verify-node-counts-in-each-cluster","text":"$ kubectl --context cluster1 get nodes $ kubectl --context cluster2 get nodes","title":"Verify node counts in each cluster"},{"location":"Installing-Submariner-on-Kind/#deploying-calico","text":"Create a folder for Calico manifests: $ mkdir calico_manifests","title":"Deploying Calico"},{"location":"Installing-Submariner-on-Kind/#install-calico-on-cluster-1","text":"$ kubectl --context cluster1 create -f https://raw.githubusercontent.com/projectcalico/calico/v3.29.0/manifests/tigera-operator.yaml $ wget -O calico_manifests/custom-resources.yaml https://raw.githubusercontent.com/projectcalico/calico/v3.29.0/manifests/custom-resources.yaml $ sed -i 's,cidr: 192.168.0.0/16,cidr: 10.130.0.0/16,g' calico_manifests/custom-resources.yaml $ sed -i 's,VXLANCrossSubnet,VXLAN,g' calico_manifests/custom-resources.yaml $ kubectl --context cluster1 apply -f calico_manifests/custom-resources.yaml","title":"Install Calico on cluster 1"},{"location":"Installing-Submariner-on-Kind/#install-calico-on-cluster-2","text":"$ kubectl --context cluster2 create -f https://raw.githubusercontent.com/projectcalico/calico/v3.29.0/manifests/tigera-operator.yaml $ wget -O calico_manifests/custom-resources.yaml https://raw.githubusercontent.com/projectcalico/calico/v3.29.0/manifests/custom-resources.yaml $ sed -i 's,cidr: 192.168.0.0/16,cidr: 10.131.0.0/16,g' calico_manifests/custom-resources.yaml $ sed -i 's,VXLANCrossSubnet,VXLAN,g' calico_manifests/custom-resources.yaml $ kubectl --context cluster2 apply -f calico_manifests/custom-resources.yaml","title":"Install Calico on cluster 2"},{"location":"Installing-Submariner-on-Kind/#deploying-submariner","text":"","title":"Deploying Submariner"},{"location":"Installing-Submariner-on-Kind/#deploying-the-broker","text":"$ subctl deploy-broker --context cluster1","title":"Deploying the broker"},{"location":"Installing-Submariner-on-Kind/#connecting-clusters-to-the-broker","text":"$ subctl join --context cluster1 broker-info.subm --clusterid cluster1 --natt = false $ subctl join --context cluster2 broker-info.subm --clusterid cluster2 --natt = false","title":"Connecting clusters to the broker"},{"location":"Installing-Submariner-on-Kind/#checking-submariner-connections","text":"$ subctl show connections --context cluster2 $ subctl show connections --context cluster1","title":"Checking Submariner connections"},{"location":"Installing-Submariner-on-OpenShift/","text":"Installing Submariner on OpenShift This section explains setting up Submariner with OpenShift clusters using the GlobalNet controller. Before you begin The following steps assume that you are creating a stretch cluster across three OpenShift clusters and have created three separate kubeconfig files, one for each OpenShift cluster. Each file should only contain a reference to a single cluster. Only one of these clusters will host the Submariner broker. Deploying the broker Deploy the broker to one OpenShift cluster. You specify the OpenShift cluster that will host the broker by passing the appropriate kubeconfig file to the command: $ subctl deploy-broker --globalnet --kubeconfig config-str2-a Once deployed, a broker-info.subm file will be generated containing authentication and connection details. Globalnet Most OpenShift clusters created using automation jobs share the same Pod and Service CIDRs. To prevent conflicts, the GlobalNet controller assigns unique, non-overlapping IPs to each cluster. Joining clusters to the broker Assign a unique clusterid to each OpenShift cluster: Cluster-id These clusterid values must match the values used in the KafkaNodePool (KNP) CR when deploying Strimzi in stretch mode. $ subctl join --kubeconfig config-str2-a --clusterid cluster1 broker-info.subm --check-broker-certificate = false $ subctl join --kubeconfig config-str2-b --clusterid cluster2 broker-info.subm --check-broker-certificate = false $ subctl join --kubeconfig config-str2-c --clusterid cluster3 broker-info.subm --check-broker-certificate = false Next steps Check if the clusters are correctly connected .","title":"Installing Submariner on OpenShift"},{"location":"Installing-Submariner-on-OpenShift/#installing-submariner-on-openshift","text":"This section explains setting up Submariner with OpenShift clusters using the GlobalNet controller.","title":"Installing Submariner on OpenShift"},{"location":"Installing-Submariner-on-OpenShift/#before-you-begin","text":"The following steps assume that you are creating a stretch cluster across three OpenShift clusters and have created three separate kubeconfig files, one for each OpenShift cluster. Each file should only contain a reference to a single cluster. Only one of these clusters will host the Submariner broker.","title":"Before you begin"},{"location":"Installing-Submariner-on-OpenShift/#deploying-the-broker","text":"Deploy the broker to one OpenShift cluster. You specify the OpenShift cluster that will host the broker by passing the appropriate kubeconfig file to the command: $ subctl deploy-broker --globalnet --kubeconfig config-str2-a Once deployed, a broker-info.subm file will be generated containing authentication and connection details. Globalnet Most OpenShift clusters created using automation jobs share the same Pod and Service CIDRs. To prevent conflicts, the GlobalNet controller assigns unique, non-overlapping IPs to each cluster.","title":"Deploying the broker"},{"location":"Installing-Submariner-on-OpenShift/#joining-clusters-to-the-broker","text":"Assign a unique clusterid to each OpenShift cluster: Cluster-id These clusterid values must match the values used in the KafkaNodePool (KNP) CR when deploying Strimzi in stretch mode. $ subctl join --kubeconfig config-str2-a --clusterid cluster1 broker-info.subm --check-broker-certificate = false $ subctl join --kubeconfig config-str2-b --clusterid cluster2 broker-info.subm --check-broker-certificate = false $ subctl join --kubeconfig config-str2-c --clusterid cluster3 broker-info.subm --check-broker-certificate = false","title":"Joining clusters to the broker"},{"location":"Installing-Submariner-on-OpenShift/#next-steps","text":"Check if the clusters are correctly connected .","title":"Next steps"},{"location":"Secure-remote-cluster-credentials/","text":"Securing Remote Cluster Credentials in Multi-Cluster Kafka Deployment Overview In a multi-cluster Kafka deployment, the central Strimzi operator requires access to multiple remote Kubernetes clusters to manage Kafka nodes across them. To achieve this, users must provide the necessary credentials in Kubernetes Secrets, ensuring secure authentication while following best practices for least-privilege access. Defining Remote Cluster Credentials The Strimzi operator in the central cluster references remote cluster credentials through STRIMZI_K8S_CLUSTERS environment variable in the central cluster's operator deployment. - name : STRIMZI_K8S_CLUSTERS value : | cluster-id-a.url=<cluster-a URL> cluster-id-a.secret=<secret-name-cluster-a> cluster-id-b.url=<cluster-b URL> cluster-id-b.secret=<secret-name-cluster-b> Each referenced secret must contain a kubeconfig file under the kubeconfig key, which provides authentication details for the corresponding remote cluster. Recommended Security Practices 1. Use a Dedicated ServiceAccount with Scoped Permissions Instead of embedding full administrator credentials in the kubeconfig, create a dedicated ServiceAccount in each remote cluster with only the necessary permissions. Example apiVersion : v1 kind : ServiceAccount metadata : name : stretch-operator namespace : <namespace> --- apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : stretch-operator-role namespace : <namespace> rules : - apiGroups : - core.strimzi.io resources : - strimzipodsets verbs : - get - list - watch - create - update - patch - delete - apiGroups : - \"\" resources : - services - configmaps - secrets - serviceaccounts verbs : - get - list - watch - create - update - patch - delete - apiGroups : - multicluster.x-k8s.io resources : - serviceexports verbs : - get - list - watch - create - update - patch - delete --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : stretch-operator-rolebinding namespace : <namespace> roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : stretch-operator-role subjects : - kind : ServiceAccount name : stretch-operator namespace : <namespace> 2. Use Token-Based Authentication To securely authenticate the Strimzi operator with remote clusters, use a long-lived token associated with the dedicated ServiceAccount. Generating a Long-Lived Token apiVersion : v1 kind : Secret metadata : name : my-long-lived-secret annotations : kubernetes.io/service-account.name : stretch-operator type : kubernetes.io/service-account-token 3. Store Minimal Credentials in Kubeconfig Instead of using a full administrator kubeconfig, generate a scoped kubeconfig with only the necessary permissions. Example Scoped Kubeconfig apiVersion : v1 clusters : - cluster : insecure-skip-tls-verify : true server : https://api.example.com:6443 name : example-cluster contexts : - context : cluster : example-cluster namespace : <namespace> user : stretch-operator-user name : stretch-cluster-context current-context : stretch-cluster-context kind : Config preferences : {} users : - name : stretch-operator-user user : token : <TOKEN> Alternative Authentication Mechanisms While token-based authentication is one of the possible approach, users may choose alternative authentication mechanisms based on their security policies: mTLS authentication: Using client TLS certificates instead of tokens in the kubeconfig. Username/password authentication: With integration into external authentication providers. OIDC-based authentication: Leveraging identity providers for federated authentication. The Strimzi operator remains agnostic to the authentication method, relying only on the kubeconfig provided by the user. Benefits of Secure Remote Authentication Principle of Least Privilege: Operators can only access the required resources. Long-Lived Credentials: Prevents frequent authentication failures. Flexible Authentication Options: Users can integrate existing security models without modifying the operator. By following these best practices, users can securely manage remote cluster credentials while ensuring a robust and secure multi-cluster Kafka deployment with Strimzi.","title":"Securing Remote Cluster Credentials in Multi-Cluster Kafka Deployment"},{"location":"Secure-remote-cluster-credentials/#securing-remote-cluster-credentials-in-multi-cluster-kafka-deployment","text":"","title":"Securing Remote Cluster Credentials in Multi-Cluster Kafka Deployment"},{"location":"Secure-remote-cluster-credentials/#overview","text":"In a multi-cluster Kafka deployment, the central Strimzi operator requires access to multiple remote Kubernetes clusters to manage Kafka nodes across them. To achieve this, users must provide the necessary credentials in Kubernetes Secrets, ensuring secure authentication while following best practices for least-privilege access.","title":"Overview"},{"location":"Secure-remote-cluster-credentials/#defining-remote-cluster-credentials","text":"The Strimzi operator in the central cluster references remote cluster credentials through STRIMZI_K8S_CLUSTERS environment variable in the central cluster's operator deployment. - name : STRIMZI_K8S_CLUSTERS value : | cluster-id-a.url=<cluster-a URL> cluster-id-a.secret=<secret-name-cluster-a> cluster-id-b.url=<cluster-b URL> cluster-id-b.secret=<secret-name-cluster-b> Each referenced secret must contain a kubeconfig file under the kubeconfig key, which provides authentication details for the corresponding remote cluster.","title":"Defining Remote Cluster Credentials"},{"location":"Secure-remote-cluster-credentials/#recommended-security-practices","text":"","title":"Recommended Security Practices"},{"location":"Secure-remote-cluster-credentials/#1-use-a-dedicated-serviceaccount-with-scoped-permissions","text":"Instead of embedding full administrator credentials in the kubeconfig, create a dedicated ServiceAccount in each remote cluster with only the necessary permissions.","title":"1. Use a Dedicated ServiceAccount with Scoped Permissions"},{"location":"Secure-remote-cluster-credentials/#example","text":"apiVersion : v1 kind : ServiceAccount metadata : name : stretch-operator namespace : <namespace> --- apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : stretch-operator-role namespace : <namespace> rules : - apiGroups : - core.strimzi.io resources : - strimzipodsets verbs : - get - list - watch - create - update - patch - delete - apiGroups : - \"\" resources : - services - configmaps - secrets - serviceaccounts verbs : - get - list - watch - create - update - patch - delete - apiGroups : - multicluster.x-k8s.io resources : - serviceexports verbs : - get - list - watch - create - update - patch - delete --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : stretch-operator-rolebinding namespace : <namespace> roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : stretch-operator-role subjects : - kind : ServiceAccount name : stretch-operator namespace : <namespace>","title":"Example"},{"location":"Secure-remote-cluster-credentials/#2-use-token-based-authentication","text":"To securely authenticate the Strimzi operator with remote clusters, use a long-lived token associated with the dedicated ServiceAccount.","title":"2. Use Token-Based Authentication"},{"location":"Secure-remote-cluster-credentials/#generating-a-long-lived-token","text":"apiVersion : v1 kind : Secret metadata : name : my-long-lived-secret annotations : kubernetes.io/service-account.name : stretch-operator type : kubernetes.io/service-account-token","title":"Generating a Long-Lived Token"},{"location":"Secure-remote-cluster-credentials/#3-store-minimal-credentials-in-kubeconfig","text":"Instead of using a full administrator kubeconfig, generate a scoped kubeconfig with only the necessary permissions.","title":"3. Store Minimal Credentials in Kubeconfig"},{"location":"Secure-remote-cluster-credentials/#example-scoped-kubeconfig","text":"apiVersion : v1 clusters : - cluster : insecure-skip-tls-verify : true server : https://api.example.com:6443 name : example-cluster contexts : - context : cluster : example-cluster namespace : <namespace> user : stretch-operator-user name : stretch-cluster-context current-context : stretch-cluster-context kind : Config preferences : {} users : - name : stretch-operator-user user : token : <TOKEN>","title":"Example Scoped Kubeconfig"},{"location":"Secure-remote-cluster-credentials/#alternative-authentication-mechanisms","text":"While token-based authentication is one of the possible approach, users may choose alternative authentication mechanisms based on their security policies: mTLS authentication: Using client TLS certificates instead of tokens in the kubeconfig. Username/password authentication: With integration into external authentication providers. OIDC-based authentication: Leveraging identity providers for federated authentication. The Strimzi operator remains agnostic to the authentication method, relying only on the kubeconfig provided by the user.","title":"Alternative Authentication Mechanisms"},{"location":"Secure-remote-cluster-credentials/#benefits-of-secure-remote-authentication","text":"Principle of Least Privilege: Operators can only access the required resources. Long-Lived Credentials: Prevents frequent authentication failures. Flexible Authentication Options: Users can integrate existing security models without modifying the operator. By following these best practices, users can securely manage remote cluster credentials while ensuring a robust and secure multi-cluster Kafka deployment with Strimzi.","title":"Benefits of Secure Remote Authentication"},{"location":"Setting-up-Rack-Awareness-In-Stretch-Cluster/","text":"Rack Awareness in Stretch Clusters Overview Kubernetes uses zone-aware scheduling to distribute pod replicas across different zones for improved fault tolerance. Some Kubernetes clusters, such as AWS-based clusters, already have built-in zone awareness. However, for clusters that are not zone-aware, each worker node must be manually labeled with a zone. Configuring Rack Awareness To enable rack awareness in a Stretch Kafka cluster, follow these steps: Step 1: Label Worker Nodes Label each Kubernetes worker node with its respective zone using the following command: \ud83d\udd25\ud83d\udd25\ud83d\udd25 $ kubectl label node <workernodename> topology.kubernetes.io/zone = <value> Repeat this command for all worker nodes in each cluster involved in the Stretch setup. Example: Central Cluster \ud83d\udd25\ud83d\udd25\ud83d\udd25 $ kubectl get node --selector = '!node-role.kubernetes.io/master' --show-labels | awk '{print $6}' Output: beta.kubernetes.io/arch = amd64,beta.kubernetes.io/os = linux,kubernetes.io/arch = amd64,kubernetes.io/hostname = worker0.example.com,kubernetes.io/os = linux,node-role.kubernetes.io/worker = ,topology.kubernetes.io/zone = cluster1 beta.kubernetes.io/arch = amd64,beta.kubernetes.io/os = linux,kubernetes.io/arch = amd64,kubernetes.io/hostname = worker1.example.com,kubernetes.io/os = linux,node-role.kubernetes.io/worker = ,topology.kubernetes.io/zone = cluster1 Example: Remote Clusters \ud83d\udd25\ud83d\udd25\ud83d\udd25 $ kubectl get node --kubeconfig cluster-a-config --selector = '!node-role.kubernetes.io/master' --show-labels | awk '{print $6}' Output: beta.kubernetes.io/arch = amd64,beta.kubernetes.io/os = linux,kubernetes.io/arch = amd64,kubernetes.io/hostname = worker0.remote1.example.com,kubernetes.io/os = linux,node-role.kubernetes.io/worker = ,topology.kubernetes.io/zone = cluster2 Repeat the above steps for all remote clusters involved. Step 2: Update Kafka Custom Resource Once all worker nodes are labeled, update the Kafka CR in the Central Cluster to include rack awareness settings: spec : kafka : rack : topologyKey : topology.kubernetes.io/zone Step 3: Configure ClusterRoleBinding Strimzi creates a ClusterRoleBinding in the Central Cluster to allow necessary permissions for fetching node details. This must be manually created in the remote clusters for now ( Note : Once we start real implementation we can make code changes such that this ClusterRoleBinding will be automatically created in the remote cluster by the Operator running in the central cluster ) kind : ClusterRoleBinding apiVersion : rbac.authorization.k8s.io/v1 metadata : name : strimzi-my-cluster-kafka-init labels : app.kubernetes.io/instance : my-cluster app.kubernetes.io/managed-by : strimzi-cluster-operator app.kubernetes.io/name : kafka app.kubernetes.io/part-of : strimzi-my-cluster strimzi.io/cluster : my-cluster strimzi.io/component-type : kafka strimzi.io/kind : Kafka strimzi.io/name : my-cluster-kafka subjects : - kind : ServiceAccount name : my-cluster-kafka namespace : kafka-namespace roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : strimzi-kafka-broker Verifying Rack Awareness Create a Topic \ud83d\udd25\ud83d\udd25\ud83d\udd25 $ bin/kafka-topics.sh --create --bootstrap-server my-cluster-kafka-bootstrap.kafka-namespace.svc.cluster.local:9092 --replication-factor 3 --partitions 6 --topic test-topic-1 Describe the Topic \ud83d\udd25\ud83d\udd25\ud83d\udd25 $ bin/kafka-topics.sh --describe --bootstrap-server my-cluster-kafka-bootstrap.kafka-namespace.svc.cluster.local:9092 --topic test-topic-1 Example Output: Topic: test-topic-1 PartitionCount: 6 ReplicationFactor: 3 Partition: 0 Leader: 2 Replicas: 2 ,14,6 Isr: 2 ,14,6 Partition: 1 Leader: 12 Replicas: 12 ,7,0 Isr: 12 ,7,0 Partition: 2 Leader: 8 Replicas: 8 ,1,13 Isr: 8 ,1,13 Check Broker Configuration \ud83d\udd25\ud83d\udd25\ud83d\udd25 $ cat /opt/kafka/init/rack.id Output for brokers in different clusters: cluster1 cluster2 cluster3 Verify Broker ConfigMaps Contain Rack ID Settings ########## # Rack ID ########## broker.rack=${strimzidir:/opt/kafka/init:rack.id} Strimzi typically sets this environment variable in the broker pod configMap based on rackId property in the Kafka CR. Verify rack details using kafka-broker-api-versions.sh sh-5.1$ bin/kafka-broker-api-versions.sh --bootstrap-server my-cluster-kafka-bootstrap.real.svc:9092 my-cluster-stretch2-broker-12.cluster2.my-cluster-kafka-brokers.real.svc.clusterset.local:9092 ( id: 12 rack: cluster3 ) -> ( Produce ( 0 ) : 0 to 11 [ usable: 11 ] , ...... ) my-cluster-stretch2-broker-13.cluster2.my-cluster-kafka-brokers.real.svc.clusterset.local:9092 ( id: 13 rack: cluster3 ) -> ( Produce ( 0 ) : 0 to 11 [ usable: 11 ] , ...... ) my-cluster-stretch1-broker-7.cluster3.my-cluster-kafka-brokers.real.svc.clusterset.local:9092 ( id: 7 rack: cluster1 ) -> ( Produce ( 0 ) : 0 to 11 [ usable: 11 ] , ...... ) my-cluster-broker-1.cluster1.my-cluster-kafka-brokers.real.svc.clusterset.local:9092 ( id: 1 rack: cluster2 ) -> ( Produce ( 0 ) : 0 to 11 [ usable: 11 ] , ...... ) my-cluster-stretch1-broker-6.cluster3.my-cluster-kafka-brokers.real.svc.clusterset.local:9092 ( id: 6 rack: cluster1 ) -> ( ...... ) my-cluster-stretch1-broker-8.cluster3.my-cluster-kafka-brokers.real.svc.clusterset.local:9092 ( id: 8 rack: cluster1 ) -> ( Produce ( 0 ) : 0 to 11 [ usable: 11 ] , ...... ) my-cluster-broker-0.cluster1.my-cluster-kafka-brokers.real.svc.clusterset.local:9092 ( id: 0 rack: cluster2 ) -> ( Produce ( 0 ) : 0 to 11 [ usable: 11 ] , ...... ) my-cluster-broker-2.cluster1.my-cluster-kafka-brokers.real.svc.clusterset.local:9092 ( id: 2 rack: cluster2 ) -> ( Produce ( 0 ) : 0 to 11 [ usable: 11 ] , ...... ) my-cluster-stretch2-broker-14.cluster2.my-cluster-kafka-brokers.real.svc.clusterset.local:9092 ( id: 14 rack: cluster3 ) -> ( Produce ( 0 ) : 0 to 11 [ usable: 11 ] , ...... ) sh-5.1$ This command lists all Kafka brokers along with their rack information. The rack ID is displayed next to each broker (rack: clusterX), confirming that the rack-aware configuration is correctly applied. With this setup, Kafka can distribute replicas across different zones, ensuring resilience in a multi-cluster deployment.","title":"Rack Awareness in Stretch Clusters"},{"location":"Setting-up-Rack-Awareness-In-Stretch-Cluster/#rack-awareness-in-stretch-clusters","text":"","title":"Rack Awareness in Stretch Clusters"},{"location":"Setting-up-Rack-Awareness-In-Stretch-Cluster/#overview","text":"Kubernetes uses zone-aware scheduling to distribute pod replicas across different zones for improved fault tolerance. Some Kubernetes clusters, such as AWS-based clusters, already have built-in zone awareness. However, for clusters that are not zone-aware, each worker node must be manually labeled with a zone.","title":"Overview"},{"location":"Setting-up-Rack-Awareness-In-Stretch-Cluster/#configuring-rack-awareness","text":"To enable rack awareness in a Stretch Kafka cluster, follow these steps:","title":"Configuring Rack Awareness"},{"location":"Setting-up-Rack-Awareness-In-Stretch-Cluster/#step-1-label-worker-nodes","text":"Label each Kubernetes worker node with its respective zone using the following command: \ud83d\udd25\ud83d\udd25\ud83d\udd25 $ kubectl label node <workernodename> topology.kubernetes.io/zone = <value> Repeat this command for all worker nodes in each cluster involved in the Stretch setup. Example: Central Cluster \ud83d\udd25\ud83d\udd25\ud83d\udd25 $ kubectl get node --selector = '!node-role.kubernetes.io/master' --show-labels | awk '{print $6}' Output: beta.kubernetes.io/arch = amd64,beta.kubernetes.io/os = linux,kubernetes.io/arch = amd64,kubernetes.io/hostname = worker0.example.com,kubernetes.io/os = linux,node-role.kubernetes.io/worker = ,topology.kubernetes.io/zone = cluster1 beta.kubernetes.io/arch = amd64,beta.kubernetes.io/os = linux,kubernetes.io/arch = amd64,kubernetes.io/hostname = worker1.example.com,kubernetes.io/os = linux,node-role.kubernetes.io/worker = ,topology.kubernetes.io/zone = cluster1 Example: Remote Clusters \ud83d\udd25\ud83d\udd25\ud83d\udd25 $ kubectl get node --kubeconfig cluster-a-config --selector = '!node-role.kubernetes.io/master' --show-labels | awk '{print $6}' Output: beta.kubernetes.io/arch = amd64,beta.kubernetes.io/os = linux,kubernetes.io/arch = amd64,kubernetes.io/hostname = worker0.remote1.example.com,kubernetes.io/os = linux,node-role.kubernetes.io/worker = ,topology.kubernetes.io/zone = cluster2 Repeat the above steps for all remote clusters involved.","title":"Step 1: Label Worker Nodes"},{"location":"Setting-up-Rack-Awareness-In-Stretch-Cluster/#step-2-update-kafka-custom-resource","text":"Once all worker nodes are labeled, update the Kafka CR in the Central Cluster to include rack awareness settings: spec : kafka : rack : topologyKey : topology.kubernetes.io/zone","title":"Step 2: Update Kafka Custom Resource"},{"location":"Setting-up-Rack-Awareness-In-Stretch-Cluster/#step-3-configure-clusterrolebinding","text":"Strimzi creates a ClusterRoleBinding in the Central Cluster to allow necessary permissions for fetching node details. This must be manually created in the remote clusters for now ( Note : Once we start real implementation we can make code changes such that this ClusterRoleBinding will be automatically created in the remote cluster by the Operator running in the central cluster ) kind : ClusterRoleBinding apiVersion : rbac.authorization.k8s.io/v1 metadata : name : strimzi-my-cluster-kafka-init labels : app.kubernetes.io/instance : my-cluster app.kubernetes.io/managed-by : strimzi-cluster-operator app.kubernetes.io/name : kafka app.kubernetes.io/part-of : strimzi-my-cluster strimzi.io/cluster : my-cluster strimzi.io/component-type : kafka strimzi.io/kind : Kafka strimzi.io/name : my-cluster-kafka subjects : - kind : ServiceAccount name : my-cluster-kafka namespace : kafka-namespace roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : strimzi-kafka-broker","title":"Step 3: Configure ClusterRoleBinding"},{"location":"Setting-up-Rack-Awareness-In-Stretch-Cluster/#verifying-rack-awareness","text":"","title":"Verifying Rack Awareness"},{"location":"Setting-up-Rack-Awareness-In-Stretch-Cluster/#create-a-topic","text":"\ud83d\udd25\ud83d\udd25\ud83d\udd25 $ bin/kafka-topics.sh --create --bootstrap-server my-cluster-kafka-bootstrap.kafka-namespace.svc.cluster.local:9092 --replication-factor 3 --partitions 6 --topic test-topic-1","title":"Create a Topic"},{"location":"Setting-up-Rack-Awareness-In-Stretch-Cluster/#describe-the-topic","text":"\ud83d\udd25\ud83d\udd25\ud83d\udd25 $ bin/kafka-topics.sh --describe --bootstrap-server my-cluster-kafka-bootstrap.kafka-namespace.svc.cluster.local:9092 --topic test-topic-1 Example Output: Topic: test-topic-1 PartitionCount: 6 ReplicationFactor: 3 Partition: 0 Leader: 2 Replicas: 2 ,14,6 Isr: 2 ,14,6 Partition: 1 Leader: 12 Replicas: 12 ,7,0 Isr: 12 ,7,0 Partition: 2 Leader: 8 Replicas: 8 ,1,13 Isr: 8 ,1,13","title":"Describe the Topic"},{"location":"Setting-up-Rack-Awareness-In-Stretch-Cluster/#check-broker-configuration","text":"\ud83d\udd25\ud83d\udd25\ud83d\udd25 $ cat /opt/kafka/init/rack.id Output for brokers in different clusters: cluster1 cluster2 cluster3","title":"Check Broker Configuration"},{"location":"Setting-up-Rack-Awareness-In-Stretch-Cluster/#verify-broker-configmaps-contain-rack-id-settings","text":"########## # Rack ID ########## broker.rack=${strimzidir:/opt/kafka/init:rack.id} Strimzi typically sets this environment variable in the broker pod configMap based on rackId property in the Kafka CR.","title":"Verify Broker ConfigMaps Contain Rack ID Settings"},{"location":"Setting-up-Rack-Awareness-In-Stretch-Cluster/#verify-rack-details-using-kafka-broker-api-versionssh","text":"sh-5.1$ bin/kafka-broker-api-versions.sh --bootstrap-server my-cluster-kafka-bootstrap.real.svc:9092 my-cluster-stretch2-broker-12.cluster2.my-cluster-kafka-brokers.real.svc.clusterset.local:9092 ( id: 12 rack: cluster3 ) -> ( Produce ( 0 ) : 0 to 11 [ usable: 11 ] , ...... ) my-cluster-stretch2-broker-13.cluster2.my-cluster-kafka-brokers.real.svc.clusterset.local:9092 ( id: 13 rack: cluster3 ) -> ( Produce ( 0 ) : 0 to 11 [ usable: 11 ] , ...... ) my-cluster-stretch1-broker-7.cluster3.my-cluster-kafka-brokers.real.svc.clusterset.local:9092 ( id: 7 rack: cluster1 ) -> ( Produce ( 0 ) : 0 to 11 [ usable: 11 ] , ...... ) my-cluster-broker-1.cluster1.my-cluster-kafka-brokers.real.svc.clusterset.local:9092 ( id: 1 rack: cluster2 ) -> ( Produce ( 0 ) : 0 to 11 [ usable: 11 ] , ...... ) my-cluster-stretch1-broker-6.cluster3.my-cluster-kafka-brokers.real.svc.clusterset.local:9092 ( id: 6 rack: cluster1 ) -> ( ...... ) my-cluster-stretch1-broker-8.cluster3.my-cluster-kafka-brokers.real.svc.clusterset.local:9092 ( id: 8 rack: cluster1 ) -> ( Produce ( 0 ) : 0 to 11 [ usable: 11 ] , ...... ) my-cluster-broker-0.cluster1.my-cluster-kafka-brokers.real.svc.clusterset.local:9092 ( id: 0 rack: cluster2 ) -> ( Produce ( 0 ) : 0 to 11 [ usable: 11 ] , ...... ) my-cluster-broker-2.cluster1.my-cluster-kafka-brokers.real.svc.clusterset.local:9092 ( id: 2 rack: cluster2 ) -> ( Produce ( 0 ) : 0 to 11 [ usable: 11 ] , ...... ) my-cluster-stretch2-broker-14.cluster2.my-cluster-kafka-brokers.real.svc.clusterset.local:9092 ( id: 14 rack: cluster3 ) -> ( Produce ( 0 ) : 0 to 11 [ usable: 11 ] , ...... ) sh-5.1$ This command lists all Kafka brokers along with their rack information. The rack ID is displayed next to each broker (rack: clusterX), confirming that the rack-aware configuration is correctly applied. With this setup, Kafka can distribute replicas across different zones, ensuring resilience in a multi-cluster deployment.","title":"Verify rack details using kafka-broker-api-versions.sh"},{"location":"Setting-up-cilium/","text":"Setting up Cilium This page outlines the steps required to install and configure Cilium for stretch clusters using Calico as the CNI across three Kubernetes clusters. Prerequisites A LoadBalancer address must be available for the Cilium API mesh server. Calico CNI should be installed on all clusters. Non-conflicting IP space across clusters. Step 1: Install Cilium on the first cluster cilium install --set cluster.name = cluster1 --set cluster.id = 1 --context kubernetes-admin@stretch-calico-1 Step 2: Copy the Cilium CA to other clusters for mutual TLS kubectl --context = kubernetes-admin@stretch-calico-1 get secret -n kube-system cilium-ca -o yaml | \\ kubectl --context = kubernetes-admin@stretch-calico-2 create -f - kubectl --context = kubernetes-admin@stretch-calico-1 get secret -n kube-system cilium-ca -o yaml | \\ kubectl --context = kubernetes-admin@stretch-calico-3 create -f - Step 3: Install Cilium on the remaining clusters cilium install --set cluster.name = cluster2 --set cluster.id = 2 --context kubernetes-admin@stretch-calico-2 cilium install --set cluster.name = cluster3 --set cluster.id = 3 --context kubernetes-admin@stretch-calico-3 Step 4: Verify the Cilium installation cilium status --context kubernetes-admin@stretch-calico-1 --wait cilium status --context kubernetes-admin@stretch-calico-2 --wait cilium status --context kubernetes-admin@stretch-calico-3 --wait All 3 clusters should show all resources \"OK\" Step 5: Enabling a Cluster Mesh To enable communication between clusters, use the following command to expose the ClusterMesh API as a LoadBalancer service: cilium clustermesh enable --context kubernetes-admin@stretch-calico-1 --service-type LoadBalancer cilium clustermesh enable --context kubernetes-admin@stretch-calico-2 --service-type LoadBalancer cilium clustermesh enable --context kubernetes-admin@stretch-calico-3 --service-type LoadBalancer Step 6: Verify the status of the Cluster Mesh cilium clustermesh status --context kubernetes-admin@stretch-calico-1 --wait cilium clustermesh status --context kubernetes-admin@stretch-calico-2 --wait cilium clustermesh status --context kubernetes-admin@stretch-calico-3 --wait All three clusters should show \"OK\" for all resources. Step 7: Connecting the clusters cilium clustermesh connect --context kubernetes-admin@stretch-calico-1 --destination-context kubernetes-admin@stretch-calico-2 cilium clustermesh connect --context kubernetes-admin@stretch-calico-1 --destination-context kubernetes-admin@stretch-calico-3 cilium clustermesh connect --context kubernetes-admin@stretch-calico-2 --destination-context kubernetes-admin@stretch-calico-3 Step 8: Verify cluster connectivity cilium clustermesh status --context kubernetes-admin@stretch-calico-2 --wait This command should show connections to the other two clusters. Step 9: Testing pod connectivity Try to ping the pods from one cluster to the other using IP address. Ideally, use and nginx server and webclient to test this connectivity. Setting Up CoreDNS for multi-cluster DNS resolution Exposing CoreDNS as NodePort Service Cilium Cluster Mesh does not provide multi-cluster DNS resolution for headless services by default. We can modify CoreDNS to enable this functionality. Exposing CoreDNS to all three clusters Create a NodePort service to expose CoreDNS across all three clusters: apiVersion : v1 kind : Service metadata : labels : k8s-app : core-dns-nodeport kubernetes.io/name : CoreDNS name : core-dns-nodeport namespace : kube-system spec : ports : - name : dns port : 53 protocol : UDP targetPort : 53 nodePort : 30053 - name : dns-tcp port : 53 protocol : TCP targetPort : 53 nodePort : 30053 selector : k8s-app : kube-dns sessionAffinity : None type : NodePort Apply this service to all three clusters kubectl --context = kubernetes-admin@stretch-calico-1 apply -f core-dns-nodeport.yaml kubectl --context = kubernetes-admin@stretch-calico-2 apply -f core-dns-nodeport.yaml kubectl --context = kubernetes-admin@stretch-calico-3 apply -f core-dns-nodeport.yaml Here, the DNS service is exposed on port 30053 of infraIP (xx.xx.xx.xx:30053). Setting up CoreDNS ConfigMap to forward requests We'll use a service address convention to determine the cluster details of a pod based on its service address, such as: my-cluster-broker-100.cluster1.my-cluster-kafka-brokers.strimzi.svc.cluster.local Here, cluster1 is injected into the address to identify which DNS service should resolve it. Editing the CoreDNS ConfigMap kubectl edit cm coredns -n kube-system --kubeconfig calico-1 Set up the rules as shown: apiVersion : v1 data : Corefile : | cluster2.svc.cluster.local.:53 { rewrite stop { name substring cluster2.svc.cluster.local svc.cluster.local answer auto } forward . x.xx.xx.34:30053 { expire 10s policy round_robin } cache 10 } cluster3.svc.cluster.local.:53 { rewrite stop { name substring cluster3.svc.cluster.local svc.cluster.local answer auto } forward . x.xx.xx.149:30053 { expire 10s policy round_robin } cache 10 } .:53 { errors health { lameduck 5s } ready rewrite stop { name substring cluster1.svc.cluster.local svc.cluster.local answer auto } template IN ANY clusterset.local { match \"(?P<broker>[a-zA-Z0-9-]+)\\.(?P<cluster>[a-zA-Z0-9-]+)\\.(?P<service>[a-zA-Z0-9-]+)\\.(?P<ns>[a-zA-Z0-9-]+)\\.svc\\.clusterset\\.local.$\" answer \"{{ .Name }} 60 IN CNAME {{ .Group.broker }}.{{ .Group.service }}.{{ .Group.ns }}.{{ .Group.cluster }}.svc.cluster.local\" } kubernetes cluster.local in-addr.arpa ip6.arpa { pods insecure fallthrough in-addr.arpa ip6.arpa ttl 30 } prometheus :9153 forward . /etc/resolv.conf { max_concurrent 1000 } cache 30 loop reload loadbalance } kind : ConfigMap metadata : name : coredns namespace : kube-system Essentially, we have added: cluster2.svc.cluster.local.:53 { rewrite stop { name substring cluster2.svc.cluster.local svc.cluster.local answer auto } forward . x.xx.xx.34:30053 { expire 10s policy round_robin } cache 10 } cluster3.svc.cluster.local.:53 { rewrite stop { name substring cluster3.svc.cluster.local svc.cluster.local answer auto } forward . x.xx.xx.149:30053 { expire 10s policy round_robin } cache 10 } When this DNS service is asked to resolve an address ending with cluster2.svc.cluster.local , it rewrites the string with a valid one ( .svc.cluster.local , removing cluster2) and forwards it to the DNS service in the 2nd cluster. The forward rule is followed by ip address of the 2nd cluster to forward all request in that category to the 2nd cluster's coreDNS service. Similarly it forwards the 3rd clusters address to the 3rd cluster for resolution. Another rewrite is added for the DNS to resolve it's own addresses by removing the cluster1 part from the request and allowing normal resolution: rewrite stop { name substring cluster1.svc.cluster.local svc.cluster.local answer auto } template IN ANY clusterset.local { match \"(?P<broker>[a-zA-Z0-9-]+)\\.(?P<cluster>[a-zA-Z0-9-]+)\\.(?P<service>[a-zA-Z0-9-]+)\\.(?P<ns>[a-zA-Z0-9-]+)\\.svc\\.clusterset\\.local.$\" answer \"{{ .Name }} 60 IN CNAME {{ .Group.broker }}.{{ .Group.service }}.{{ .Group.ns }}.{{ .Group.cluster }}.svc.cluster.local\" } The ConfigMap of cluster 2 is modified as shown: apiVersion : v1 data : Corefile : | cluster1.svc.cluster.local.:53 { rewrite stop { name substring cluster1.svc.cluster.local svc.cluster.local answer auto } forward . x.xx.xx.97:30053 { expire 10s policy round_robin } cache 10 } cluster3.svc.cluster.local.:53 { rewrite stop { name substring cluster3.svc.cluster.local svc.cluster.local answer auto } forward . x.xx.xx.149:30053 { expire 10s policy round_robin } cache 10 } .:53 { errors health { lameduck 5s } ready rewrite stop { name substring cluster2.svc.cluster.local svc.cluster.local answer auto } template IN ANY clusterset.local { match \"(?P<broker>[a-zA-Z0-9-]+)\\.(?P<cluster>[a-zA-Z0-9-]+)\\.(?P<service>[a-zA-Z0-9-]+)\\.(?P<ns>[a-zA-Z0-9-]+)\\.svc\\.clusterset\\.local.$\" answer \"{{ .Name }} 60 IN CNAME {{ .Group.broker }}.{{ .Group.service }}.{{ .Group.ns }}.{{ .Group.cluster }}.svc.cluster.local\" } kubernetes cluster.local in-addr.arpa ip6.arpa { pods insecure fallthrough in-addr.arpa ip6.arpa ttl 30 } prometheus :9153 forward . /etc/resolv.conf { max_concurrent 1000 } cache 30 loop reload loadbalance } kind : ConfigMap metadata : name : coredns namespace : kube-system This updates sets the resolving rules in reverse. All addresses ending with cluster1.svc.cluster.local will modified and sent to the DNS service in the other cluster which can resolve it normally. The addresses ending with cluster2.svc.cluster.local will be resolved by the local DNS cluster by removing the cluster2 part from the address. Cluster 3 addresses will be sent for resolution by cluster 3 DNS. Cluster 3's coreDNS is modified as shown: apiVersion : v1 data : Corefile : | cluster1.svc.cluster.local.:53 { rewrite stop { name substring cluster1.svc.cluster.local svc.cluster.local answer auto } forward . x.xx.xx.97:30053 { expire 10s policy round_robin } cache 10 } cluster2.svc.cluster.local.:53 { rewrite stop { name substring cluster2.svc.cluster.local svc.cluster.local answer auto } forward . x.xx.xx.34:30053 { expire 10s policy round_robin } cache 10 } .:53 { errors health { lameduck 5s } ready rewrite stop { name substring cluster3.svc.cluster.local svc.cluster.local answer auto } template IN ANY clusterset.local { match \"(?P<broker>[a-zA-Z0-9-]+)\\.(?P<cluster>[a-zA-Z0-9-]+)\\.(?P<service>[a-zA-Z0-9-]+)\\.(?P<ns>[a-zA-Z0-9-]+)\\.svc\\.clusterset\\.local.$\" answer \"{{ .Name }} 60 IN CNAME {{ .Group.broker }}.{{ .Group.service }}.{{ .Group.ns }}.{{ .Group.cluster }}.svc.cluster.local\" } kubernetes cluster.local in-addr.arpa ip6.arpa { pods insecure fallthrough in-addr.arpa ip6.arpa ttl 30 } prometheus :9153 forward . /etc/resolv.conf { max_concurrent 1000 } cache 30 loop reload loadbalance } kind : ConfigMap metadata : name : coredns namespace : kube-system Check DNS resolution using dig \ud83d\udd25\ud83d\udd25\ud83d\udd25 $ dig my-cluster-controller-4.cluster1.my-cluster-kafka-brokers.strimzi.svc.clusterset.local @xx.xx.xx.xx -p 30053 ; <<>> DiG 9 .10.6 <<>> my-cluster-controller-4.cluster1.my-cluster-kafka-brokers.strimzi.svc.clusterset.local @xx.xx.xx.xx -p 30053 ;; global options: +cmd ;; Got answer: ;; WARNING: .local is reserved for Multicast DNS ;; You are currently testing what happens when an mDNS query is leaked to DNS ;; ->>HEADER <<- opco de: QUERY, status: NOERROR, id: 17247 ;; flags: qr aa rd ; QUERY: 1 , ANSWER: 2 , AUTHORITY: 0 , ADDITIONAL: 1 ;; WARNING: recursion requested but not available ;; OPT PSEUDOSECTION: ; EDNS: version: 0 , flags: ; udp: 4096 ;; QUESTION SECTION: ; my-cluster-controller-4.cluster1.my-cluster-kafka-brokers.strimzi.svc.clusterset.local. IN A ;; ANSWER SECTION: my-cluster-controller-4.cluster1.my-cluster-kafka-brokers.strimzi.svc.clusterset.local. 10 IN CNAME my-cluster-controller-4.my-cluster-kafka-brokers.strimzi.cluster1.svc.cluster.local. my-cluster-controller-4.my-cluster-kafka-brokers.strimzi.cluster1.svc.cluster.local. 10 IN A 10 .0.1.73 ;; Query time: 270 msec ;; SERVER: xx.x.xx.xx#30053 ( xx.xx.xx.xx ) ;; WHEN: Fri Feb 07 14 :22:13 IST 2025 ;; MSG SIZE rcvd: 401 Test with all combinations: cluster-a address -> cluster-a DNS cluster-a address -> cluster-b DNS cluster-a address -> cluster-c DNS cluster-b address -> cluster-a DNS cluster-b address -> cluster-b DNS cluster-b address -> cluster-c DNS cluster-c address -> cluster-a DNS cluster-c address -> cluster-b DNS cluster-c address -> cluster-c DNS Now proceed with installing the operator in stretch mode where the Kafka CR's cross-cluster-type label is set to cilium : apiVersion : kafka.strimzi.io/v1beta2 kind : Kafka metadata : name : my-cluster annotations : strimzi.io/node-pools : enabled strimzi.io/kraft : enabled strimzi.io/cross-cluster-type : \"cilium\" #-- change this to cilium instead of submainer It is not necessary to edit clusterRole resources because no service exports are needed when using Cilium.","title":"Setting up Cilium"},{"location":"Setting-up-cilium/#setting-up-cilium","text":"This page outlines the steps required to install and configure Cilium for stretch clusters using Calico as the CNI across three Kubernetes clusters.","title":"Setting up Cilium"},{"location":"Setting-up-cilium/#prerequisites","text":"A LoadBalancer address must be available for the Cilium API mesh server. Calico CNI should be installed on all clusters. Non-conflicting IP space across clusters.","title":"Prerequisites"},{"location":"Setting-up-cilium/#step-1-install-cilium-on-the-first-cluster","text":"cilium install --set cluster.name = cluster1 --set cluster.id = 1 --context kubernetes-admin@stretch-calico-1","title":"Step 1: Install Cilium on the first cluster"},{"location":"Setting-up-cilium/#step-2-copy-the-cilium-ca-to-other-clusters-for-mutual-tls","text":"kubectl --context = kubernetes-admin@stretch-calico-1 get secret -n kube-system cilium-ca -o yaml | \\ kubectl --context = kubernetes-admin@stretch-calico-2 create -f - kubectl --context = kubernetes-admin@stretch-calico-1 get secret -n kube-system cilium-ca -o yaml | \\ kubectl --context = kubernetes-admin@stretch-calico-3 create -f -","title":"Step 2: Copy the Cilium CA to other clusters for mutual TLS"},{"location":"Setting-up-cilium/#step-3-install-cilium-on-the-remaining-clusters","text":"cilium install --set cluster.name = cluster2 --set cluster.id = 2 --context kubernetes-admin@stretch-calico-2 cilium install --set cluster.name = cluster3 --set cluster.id = 3 --context kubernetes-admin@stretch-calico-3","title":"Step 3: Install Cilium on the remaining clusters"},{"location":"Setting-up-cilium/#step-4-verify-the-cilium-installation","text":"cilium status --context kubernetes-admin@stretch-calico-1 --wait cilium status --context kubernetes-admin@stretch-calico-2 --wait cilium status --context kubernetes-admin@stretch-calico-3 --wait All 3 clusters should show all resources \"OK\"","title":"Step 4: Verify the Cilium installation"},{"location":"Setting-up-cilium/#step-5-enabling-a-cluster-mesh","text":"To enable communication between clusters, use the following command to expose the ClusterMesh API as a LoadBalancer service: cilium clustermesh enable --context kubernetes-admin@stretch-calico-1 --service-type LoadBalancer cilium clustermesh enable --context kubernetes-admin@stretch-calico-2 --service-type LoadBalancer cilium clustermesh enable --context kubernetes-admin@stretch-calico-3 --service-type LoadBalancer","title":"Step 5: Enabling a Cluster Mesh"},{"location":"Setting-up-cilium/#step-6-verify-the-status-of-the-cluster-mesh","text":"cilium clustermesh status --context kubernetes-admin@stretch-calico-1 --wait cilium clustermesh status --context kubernetes-admin@stretch-calico-2 --wait cilium clustermesh status --context kubernetes-admin@stretch-calico-3 --wait All three clusters should show \"OK\" for all resources.","title":"Step 6: Verify the status of the Cluster Mesh"},{"location":"Setting-up-cilium/#step-7-connecting-the-clusters","text":"cilium clustermesh connect --context kubernetes-admin@stretch-calico-1 --destination-context kubernetes-admin@stretch-calico-2 cilium clustermesh connect --context kubernetes-admin@stretch-calico-1 --destination-context kubernetes-admin@stretch-calico-3 cilium clustermesh connect --context kubernetes-admin@stretch-calico-2 --destination-context kubernetes-admin@stretch-calico-3","title":"Step 7: Connecting the clusters"},{"location":"Setting-up-cilium/#step-8-verify-cluster-connectivity","text":"cilium clustermesh status --context kubernetes-admin@stretch-calico-2 --wait This command should show connections to the other two clusters.","title":"Step 8: Verify cluster connectivity"},{"location":"Setting-up-cilium/#step-9-testing-pod-connectivity","text":"Try to ping the pods from one cluster to the other using IP address. Ideally, use and nginx server and webclient to test this connectivity.","title":"Step 9: Testing pod connectivity"},{"location":"Setting-up-cilium/#setting-up-coredns-for-multi-cluster-dns-resolution","text":"","title":"Setting Up CoreDNS for multi-cluster DNS resolution"},{"location":"Setting-up-cilium/#exposing-coredns-as-nodeport-service","text":"Cilium Cluster Mesh does not provide multi-cluster DNS resolution for headless services by default. We can modify CoreDNS to enable this functionality.","title":"Exposing CoreDNS as NodePort Service"},{"location":"Setting-up-cilium/#exposing-coredns-to-all-three-clusters","text":"Create a NodePort service to expose CoreDNS across all three clusters: apiVersion : v1 kind : Service metadata : labels : k8s-app : core-dns-nodeport kubernetes.io/name : CoreDNS name : core-dns-nodeport namespace : kube-system spec : ports : - name : dns port : 53 protocol : UDP targetPort : 53 nodePort : 30053 - name : dns-tcp port : 53 protocol : TCP targetPort : 53 nodePort : 30053 selector : k8s-app : kube-dns sessionAffinity : None type : NodePort Apply this service to all three clusters kubectl --context = kubernetes-admin@stretch-calico-1 apply -f core-dns-nodeport.yaml kubectl --context = kubernetes-admin@stretch-calico-2 apply -f core-dns-nodeport.yaml kubectl --context = kubernetes-admin@stretch-calico-3 apply -f core-dns-nodeport.yaml Here, the DNS service is exposed on port 30053 of infraIP (xx.xx.xx.xx:30053).","title":"Exposing CoreDNS to all three clusters"},{"location":"Setting-up-cilium/#setting-up-coredns-configmap-to-forward-requests","text":"We'll use a service address convention to determine the cluster details of a pod based on its service address, such as: my-cluster-broker-100.cluster1.my-cluster-kafka-brokers.strimzi.svc.cluster.local Here, cluster1 is injected into the address to identify which DNS service should resolve it.","title":"Setting up CoreDNS ConfigMap to forward requests"},{"location":"Setting-up-cilium/#editing-the-coredns-configmap","text":"kubectl edit cm coredns -n kube-system --kubeconfig calico-1 Set up the rules as shown: apiVersion : v1 data : Corefile : | cluster2.svc.cluster.local.:53 { rewrite stop { name substring cluster2.svc.cluster.local svc.cluster.local answer auto } forward . x.xx.xx.34:30053 { expire 10s policy round_robin } cache 10 } cluster3.svc.cluster.local.:53 { rewrite stop { name substring cluster3.svc.cluster.local svc.cluster.local answer auto } forward . x.xx.xx.149:30053 { expire 10s policy round_robin } cache 10 } .:53 { errors health { lameduck 5s } ready rewrite stop { name substring cluster1.svc.cluster.local svc.cluster.local answer auto } template IN ANY clusterset.local { match \"(?P<broker>[a-zA-Z0-9-]+)\\.(?P<cluster>[a-zA-Z0-9-]+)\\.(?P<service>[a-zA-Z0-9-]+)\\.(?P<ns>[a-zA-Z0-9-]+)\\.svc\\.clusterset\\.local.$\" answer \"{{ .Name }} 60 IN CNAME {{ .Group.broker }}.{{ .Group.service }}.{{ .Group.ns }}.{{ .Group.cluster }}.svc.cluster.local\" } kubernetes cluster.local in-addr.arpa ip6.arpa { pods insecure fallthrough in-addr.arpa ip6.arpa ttl 30 } prometheus :9153 forward . /etc/resolv.conf { max_concurrent 1000 } cache 30 loop reload loadbalance } kind : ConfigMap metadata : name : coredns namespace : kube-system Essentially, we have added: cluster2.svc.cluster.local.:53 { rewrite stop { name substring cluster2.svc.cluster.local svc.cluster.local answer auto } forward . x.xx.xx.34:30053 { expire 10s policy round_robin } cache 10 } cluster3.svc.cluster.local.:53 { rewrite stop { name substring cluster3.svc.cluster.local svc.cluster.local answer auto } forward . x.xx.xx.149:30053 { expire 10s policy round_robin } cache 10 } When this DNS service is asked to resolve an address ending with cluster2.svc.cluster.local , it rewrites the string with a valid one ( .svc.cluster.local , removing cluster2) and forwards it to the DNS service in the 2nd cluster. The forward rule is followed by ip address of the 2nd cluster to forward all request in that category to the 2nd cluster's coreDNS service. Similarly it forwards the 3rd clusters address to the 3rd cluster for resolution. Another rewrite is added for the DNS to resolve it's own addresses by removing the cluster1 part from the request and allowing normal resolution: rewrite stop { name substring cluster1.svc.cluster.local svc.cluster.local answer auto } template IN ANY clusterset.local { match \"(?P<broker>[a-zA-Z0-9-]+)\\.(?P<cluster>[a-zA-Z0-9-]+)\\.(?P<service>[a-zA-Z0-9-]+)\\.(?P<ns>[a-zA-Z0-9-]+)\\.svc\\.clusterset\\.local.$\" answer \"{{ .Name }} 60 IN CNAME {{ .Group.broker }}.{{ .Group.service }}.{{ .Group.ns }}.{{ .Group.cluster }}.svc.cluster.local\" } The ConfigMap of cluster 2 is modified as shown: apiVersion : v1 data : Corefile : | cluster1.svc.cluster.local.:53 { rewrite stop { name substring cluster1.svc.cluster.local svc.cluster.local answer auto } forward . x.xx.xx.97:30053 { expire 10s policy round_robin } cache 10 } cluster3.svc.cluster.local.:53 { rewrite stop { name substring cluster3.svc.cluster.local svc.cluster.local answer auto } forward . x.xx.xx.149:30053 { expire 10s policy round_robin } cache 10 } .:53 { errors health { lameduck 5s } ready rewrite stop { name substring cluster2.svc.cluster.local svc.cluster.local answer auto } template IN ANY clusterset.local { match \"(?P<broker>[a-zA-Z0-9-]+)\\.(?P<cluster>[a-zA-Z0-9-]+)\\.(?P<service>[a-zA-Z0-9-]+)\\.(?P<ns>[a-zA-Z0-9-]+)\\.svc\\.clusterset\\.local.$\" answer \"{{ .Name }} 60 IN CNAME {{ .Group.broker }}.{{ .Group.service }}.{{ .Group.ns }}.{{ .Group.cluster }}.svc.cluster.local\" } kubernetes cluster.local in-addr.arpa ip6.arpa { pods insecure fallthrough in-addr.arpa ip6.arpa ttl 30 } prometheus :9153 forward . /etc/resolv.conf { max_concurrent 1000 } cache 30 loop reload loadbalance } kind : ConfigMap metadata : name : coredns namespace : kube-system This updates sets the resolving rules in reverse. All addresses ending with cluster1.svc.cluster.local will modified and sent to the DNS service in the other cluster which can resolve it normally. The addresses ending with cluster2.svc.cluster.local will be resolved by the local DNS cluster by removing the cluster2 part from the address. Cluster 3 addresses will be sent for resolution by cluster 3 DNS. Cluster 3's coreDNS is modified as shown: apiVersion : v1 data : Corefile : | cluster1.svc.cluster.local.:53 { rewrite stop { name substring cluster1.svc.cluster.local svc.cluster.local answer auto } forward . x.xx.xx.97:30053 { expire 10s policy round_robin } cache 10 } cluster2.svc.cluster.local.:53 { rewrite stop { name substring cluster2.svc.cluster.local svc.cluster.local answer auto } forward . x.xx.xx.34:30053 { expire 10s policy round_robin } cache 10 } .:53 { errors health { lameduck 5s } ready rewrite stop { name substring cluster3.svc.cluster.local svc.cluster.local answer auto } template IN ANY clusterset.local { match \"(?P<broker>[a-zA-Z0-9-]+)\\.(?P<cluster>[a-zA-Z0-9-]+)\\.(?P<service>[a-zA-Z0-9-]+)\\.(?P<ns>[a-zA-Z0-9-]+)\\.svc\\.clusterset\\.local.$\" answer \"{{ .Name }} 60 IN CNAME {{ .Group.broker }}.{{ .Group.service }}.{{ .Group.ns }}.{{ .Group.cluster }}.svc.cluster.local\" } kubernetes cluster.local in-addr.arpa ip6.arpa { pods insecure fallthrough in-addr.arpa ip6.arpa ttl 30 } prometheus :9153 forward . /etc/resolv.conf { max_concurrent 1000 } cache 30 loop reload loadbalance } kind : ConfigMap metadata : name : coredns namespace : kube-system","title":"Editing the CoreDNS ConfigMap"},{"location":"Setting-up-cilium/#check-dns-resolution-using-dig","text":"\ud83d\udd25\ud83d\udd25\ud83d\udd25 $ dig my-cluster-controller-4.cluster1.my-cluster-kafka-brokers.strimzi.svc.clusterset.local @xx.xx.xx.xx -p 30053 ; <<>> DiG 9 .10.6 <<>> my-cluster-controller-4.cluster1.my-cluster-kafka-brokers.strimzi.svc.clusterset.local @xx.xx.xx.xx -p 30053 ;; global options: +cmd ;; Got answer: ;; WARNING: .local is reserved for Multicast DNS ;; You are currently testing what happens when an mDNS query is leaked to DNS ;; ->>HEADER <<- opco de: QUERY, status: NOERROR, id: 17247 ;; flags: qr aa rd ; QUERY: 1 , ANSWER: 2 , AUTHORITY: 0 , ADDITIONAL: 1 ;; WARNING: recursion requested but not available ;; OPT PSEUDOSECTION: ; EDNS: version: 0 , flags: ; udp: 4096 ;; QUESTION SECTION: ; my-cluster-controller-4.cluster1.my-cluster-kafka-brokers.strimzi.svc.clusterset.local. IN A ;; ANSWER SECTION: my-cluster-controller-4.cluster1.my-cluster-kafka-brokers.strimzi.svc.clusterset.local. 10 IN CNAME my-cluster-controller-4.my-cluster-kafka-brokers.strimzi.cluster1.svc.cluster.local. my-cluster-controller-4.my-cluster-kafka-brokers.strimzi.cluster1.svc.cluster.local. 10 IN A 10 .0.1.73 ;; Query time: 270 msec ;; SERVER: xx.x.xx.xx#30053 ( xx.xx.xx.xx ) ;; WHEN: Fri Feb 07 14 :22:13 IST 2025 ;; MSG SIZE rcvd: 401 Test with all combinations: cluster-a address -> cluster-a DNS cluster-a address -> cluster-b DNS cluster-a address -> cluster-c DNS cluster-b address -> cluster-a DNS cluster-b address -> cluster-b DNS cluster-b address -> cluster-c DNS cluster-c address -> cluster-a DNS cluster-c address -> cluster-b DNS cluster-c address -> cluster-c DNS Now proceed with installing the operator in stretch mode where the Kafka CR's cross-cluster-type label is set to cilium : apiVersion : kafka.strimzi.io/v1beta2 kind : Kafka metadata : name : my-cluster annotations : strimzi.io/node-pools : enabled strimzi.io/kraft : enabled strimzi.io/cross-cluster-type : \"cilium\" #-- change this to cilium instead of submainer It is not necessary to edit clusterRole resources because no service exports are needed when using Cilium.","title":"Check DNS resolution using dig"},{"location":"Stretch-Cluster-Architecture/","text":"Stretch cluster architecture In this section, we will discuss the architecture of a stretch Kafka cluster. The stretch cluster design extends the Strimzi Kafka operator to support the distribution of brokers and controllers across multiple Kubernetes clusters. The primary goal is to enhance the high availability of the Kafka data plane. The above diagram outlines the high-level topology and design concepts for such a deployment. Stretch Kafka clusters require multiple Kubernetes clusters: users can configure any number of clusters, but for simplicity, the diagram illustrates a three-cluster setup. Kafka clusters should be deployed in environments that allow low-latency communication between brokers and controllers. Stretch Kafka clusters are best suited for data centers or availability zones within a single region. They should not be deployed across geographically distant regions where high latency could degrade performance. Central vs. Member (Remote) Clusters In a stretch cluster setup: Kafka and KafkaNodePool Custom Resources (CRs) are applied in a single Kubernetes cluster, known as the \"central\" cluster. The Cluster Operator (CO) runs in the central cluster and is responsible for creating Kubernetes resources required to bring up Kafka broker and controller pods in member (remote) clusters. Cluster operators must be deployed in all Kubernetes clusters. The cluster operator in the central cluster manages all required resources across clusters. The cluster operator in remote clusters ensures that Kafka pods are restarted if they crash. Handling failures The stretch cluster architecture ensures fault tolerance even if one or more Kubernetes clusters experience an outage. If a remote cluster fails, the remaining clusters continue serving Kafka clients. Additionally, if the central cluster goes down, Kafka operations remain unaffected, as the Kafka pods in the remaining clusters continue to function and serve clients without disruption.","title":"Stretch cluster architecture"},{"location":"Stretch-Cluster-Architecture/#stretch-cluster-architecture","text":"In this section, we will discuss the architecture of a stretch Kafka cluster. The stretch cluster design extends the Strimzi Kafka operator to support the distribution of brokers and controllers across multiple Kubernetes clusters. The primary goal is to enhance the high availability of the Kafka data plane. The above diagram outlines the high-level topology and design concepts for such a deployment. Stretch Kafka clusters require multiple Kubernetes clusters: users can configure any number of clusters, but for simplicity, the diagram illustrates a three-cluster setup. Kafka clusters should be deployed in environments that allow low-latency communication between brokers and controllers. Stretch Kafka clusters are best suited for data centers or availability zones within a single region. They should not be deployed across geographically distant regions where high latency could degrade performance.","title":"Stretch cluster architecture"},{"location":"Stretch-Cluster-Architecture/#central-vs-member-remote-clusters","text":"In a stretch cluster setup: Kafka and KafkaNodePool Custom Resources (CRs) are applied in a single Kubernetes cluster, known as the \"central\" cluster. The Cluster Operator (CO) runs in the central cluster and is responsible for creating Kubernetes resources required to bring up Kafka broker and controller pods in member (remote) clusters. Cluster operators must be deployed in all Kubernetes clusters. The cluster operator in the central cluster manages all required resources across clusters. The cluster operator in remote clusters ensures that Kafka pods are restarted if they crash.","title":"Central vs. Member (Remote) Clusters"},{"location":"Stretch-Cluster-Architecture/#handling-failures","text":"The stretch cluster architecture ensures fault tolerance even if one or more Kubernetes clusters experience an outage. If a remote cluster fails, the remaining clusters continue serving Kafka clients. Additionally, if the central cluster goes down, Kafka operations remain unaffected, as the Kafka pods in the remaining clusters continue to function and serve clients without disruption.","title":"Handling failures"},{"location":"Testing-cluster-failover/","text":"Cluster Failover Testing Objective This test simulates a scenario in which the central cluster goes down. It evaluates the behavior of Kafka topics, leader election, in-sync replicas (ISRs), and how a stretched Kafka cluster handles failover. After testing the failover, the central cluster is recovered to determine if the old state is restored. Cluster setup We have three Kubernetes clusters with a stretched Kafka deployment across them. Cluster Configurations Central Cluster (calico-1) $ .kube % kubectl get pods --kubeconfig calico-1 -n strimzi NAME READY STATUS RESTARTS AGE my-cluster-broker-0 1 /1 Running 0 23h my-cluster-broker-1 1 /1 Running 0 23h my-cluster-broker-2 1 /1 Running 0 23h my-cluster-controller-3 1 /1 Running 0 23h my-cluster-controller-4 1 /1 Running 0 23h my-cluster-controller-5 1 /1 Running 0 23h strimzi-cluster-operator-5b7b9d9bf6-w4hxl 1 /1 Running 0 23h Stretch Cluster 1 (calico-2) $ .kube % kubectl get pods --kubeconfig calico-2 -n strimzi NAME READY STATUS RESTARTS AGE my-cluster-stretch1-broker-6 1 /1 Running 0 23h my-cluster-stretch1-broker-7 1 /1 Running 0 23h my-cluster-stretch1-broker-8 1 /1 Running 0 23h my-cluster-stretch1-controller-10 1 /1 Running 0 23h my-cluster-stretch1-controller-11 1 /1 Running 0 23h my-cluster-stretch1-controller-9 1 /1 Running 0 23h strimzi-cluster-operator-6d7db9dd95-k5m24 1 /1 Running 0 23h Stretch Cluster 2 (calico-3) $ .kube % kubectl get pods --kubeconfig calico-3 -n strimzi NAME READY STATUS RESTARTS AGE my-cluster-stretch2-broker-12 1 /1 Running 0 23h my-cluster-stretch2-broker-13 1 /1 Running 0 23h my-cluster-stretch2-broker-14 1 /1 Running 0 23h my-cluster-stretch2-controller-15 1 /1 Running 0 23h my-cluster-stretch2-controller-16 1 /1 Running 0 23h my-cluster-stretch2-controller-17 1 /1 Running 0 23h strimzi-cluster-operator-7966fb9659-zqfmv 1 /1 Running 0 23h The central cluster contains the Kafka and KafkaNodePool CRs: $ .kube % kubectl get kafka -n strimzi --kubeconfig calico-1 NAME DESIRED KAFKA REPLICAS DESIRED ZK REPLICAS READY METADATA STATE WARNINGS my-cluster $ .kube % kubectl get kafkanodepool -n strimzi --kubeconfig calico-1 NAME DESIRED REPLICAS ROLES NODEIDS broker 3 [ \"broker\" ] [ 0 ,1,2 ] controller 3 [ \"controller\" ] [ 3 ,4,5 ] stretch1-broker 3 [ \"broker\" ] [ 6 ,7,8 ] stretch1-controller 3 [ \"controller\" ] [ 9 ,10,11 ] stretch2-broker 3 [ \"broker\" ] [ 12 ,13,14 ] stretch2-controller 3 [ \"controller\" ] [ 15 ,16,17 ] Listing the metadata quorum Checking if the metadata reflects all brokers and controllers across clusters: [ kafka@my-cluster-broker-0 kafka ] $ bin/kafka-metadata-quorum.sh --bootstrap-server my-cluster-kafka-bootstrap.strimzi.svc:9092 describe --status ClusterId: 1RYWwDxMT8mT0lpiqtc69w LeaderId: 11 LeaderEpoch: 195814 HighWatermark: 168406 MaxFollowerLag: 0 MaxFollowerLagTimeMs: 54 CurrentVoters: [ 16 ,17,3,4,5,9,10,11,15 ] CurrentObservers: [ 0 ,1,2,6,7,8,12,13,14 ] Topic Creation and Message Testing [ kafka@my-cluster-broker-0 kafka ] $ bin/kafka-topics.sh --create --bootstrap-server my-cluster-kafka-bootstrap.strimzi.svc:9092 --replication-factor 6 --partitions 6 --topic failover-test Created topic failover-test. Describing the topics shows that partition 4 and 5 have leaders from the central cluster and partition's all ISRs contains all brokers from the central cluster. [ kafka@my-cluster-broker-0 kafka ] $ bin/kafka-topics.sh --describe --bootstrap-server my-cluster-kafka-bootstrap.strimzi.svc:9092 --topic failover-test Topic: failover-test TopicId: 7U-yMkfgT1GfJRY-DoyEhQ PartitionCount: 6 ReplicationFactor: 6 Configs: min.insync.replicas = 2 Topic: failover-test Partition: 0 Leader: 8 Replicas: 8 ,12,13,14,0,1 Isr: 8 ,12,13,14,0,1 Elr: LastKnownElr: Topic: failover-test Partition: 1 Leader: 12 Replicas: 12 ,13,14,0,1,2 Isr: 12 ,13,14,0,1,2 Elr: LastKnownElr: Topic: failover-test Partition: 2 Leader: 13 Replicas: 13 ,14,0,1,2,6 Isr: 13 ,14,0,1,2,6 Elr: LastKnownElr: Topic: failover-test Partition: 3 Leader: 14 Replicas: 14 ,0,1,2,6,7 Isr: 14 ,0,1,2,6,7 Elr: LastKnownElr: Topic: failover-test Partition: 4 Leader: 0 Replicas: 0 ,1,2,6,7,8 Isr: 0 ,1,2,6,7,8 Elr: LastKnownElr: Topic: failover-test Partition: 5 Leader: 1 Replicas: 1 ,2,6,7,8,12 Isr: 1 ,2,6,7,8,12 Elr: LastKnownElr: Producing and consuming messages from the topic [ kafka@my-cluster-broker-0 kafka ] $ bin/kafka-console-producer.sh --bootstrap-server my-cluster-kafka-bootstrap.strimzi.svc:9092 --topic failover-test >asdfasdf >this is stretch >sending data from one cluster to the other >Hello Kafka >pushing enough messages to kafka cluster >Hello world >Testing >Testing asdf [ kafka@my-cluster-stretch1-broker-8 kafka ] $ bin/kafka-console-consumer.sh --bootstrap-server my-cluster-kafka-bootstrap.strimzi.svc:9092 --topic failover-test asdfasdf this is stretch sending data from one cluster to the other Hello Kafka pushing enough messages to kafka cluster Hello world Testing Testing asdf ^CProcessed a total of 8 messages Simulating Central Cluster Failure Manually shutting down the central cluster to simulate a cluster failure $ .kube % kubectl get pods --kubeconfig calico-1 -n strimzi -v = 8 I0313 13 :57:19.830013 14803 loader.go:395 ] Config loaded from file: calico-1 I0313 13 :57:19.834745 14803 round_trippers.go:463 ] GET https://9.46.88.97:6443/api/v1/namespaces/strimzi/pods?limit = 500 I0313 13 :57:19.834762 14803 round_trippers.go:469 ] Request Headers: I0313 13 :57:19.834768 14803 round_trippers.go:473 ] Accept: application/json ; as = Table ; v = v1 ; g = meta.k8s.io,application/json ; as = Table ; v = v1beta1 ; g = meta.k8s.io,application/json I0313 13 :57:19.834771 14803 round_trippers.go:473 ] User-Agent: kubectl1.31.1/v1.31.1 ( darwin/arm64 ) kubernetes/948afe5 I0313 13 :57:49.837331 14803 round_trippers.go:574 ] Response Status: in 30002 milliseconds I0313 13 :57:49.837405 14803 round_trippers.go:577 ] Response Headers: I0313 13 :57:49.838573 14803 helpers.go:264 ] Connection error: Get https://9.46.88.97:6443/api/v1/namespaces/strimzi/pods?limit = 500 : dial tcp 9 .46.88.97:6443: i/o timeout Unable to connect to the server: dial tcp 9 .46.88.97:6443: i/o timeout Testing if we can produce and consume with the other two clusters We're using cluster 2 and 3 to produce and consume [ kafka@my-cluster-stretch2-broker-12 kafka ] $ bin/kafka-console-producer.sh --bootstrap-server my-cluster-kafka-bootstrap.strimzi.svc:9092 --topic failover-test >hello world >Seems like the data is not lost [ kafka@my-cluster-stretch1-broker-8 kafka ] $ bin/kafka-console-consumer.sh --bootstrap-server my-cluster-kafka-bootstrap.strimzi.svc:9092 --topic failover-test --from-beginning asdfasdf this is stretch sending data from one cluster to the other Hello Kafka pushing enough messages to kafka cluster Hello world Testing Testing asdf hello world Seems like the data is not lost ^CProcessed a total of 10 messages Produce and consume works fine and it seems like the previously sent messages are being retrieved properly. Checking if leader election elected new partition leaders Partition 4 and 5 which had leaders from the failed central cluster are assigned new leaders from available cluster (cluster 2). [ kafka@my-cluster-stretch2-broker-12 kafka ] $ bin/kafka-topics.sh --describe --bootstrap-server my-cluster-kafka-bootstrap.strimzi.svc:9092 --topic failover-test Topic: failover-test TopicId: 7U-yMkfgT1GfJRY-DoyEhQ PartitionCount: 6 ReplicationFactor: 6 Configs: min.insync.replicas = 2 Topic: failover-test Partition: 0 Leader: 8 Replicas: 8 ,12,13,14,0,1 Isr: 8 ,12,14,13 Elr: LastKnownElr: Topic: failover-test Partition: 1 Leader: 12 Replicas: 12 ,13,14,0,1,2 Isr: 12 ,14,13 Elr: LastKnownElr: Topic: failover-test Partition: 2 Leader: 13 Replicas: 13 ,14,0,1,2,6 Isr: 14 ,6,13 Elr: LastKnownElr: Topic: failover-test Partition: 3 Leader: 14 Replicas: 14 ,0,1,2,6,7 Isr: 14 ,6,7 Elr: LastKnownElr: Topic: failover-test Partition: 4 Leader: 6 Replicas: 0 ,1,2,6,7,8 Isr: 6 ,7,8 Elr: LastKnownElr: Topic: failover-test Partition: 5 Leader: 6 Replicas: 1 ,2,6,7,8,12 Isr: 6 ,7,8,12 Elr: LastKnownElr: Recovering the central cluster Manually recovering the central cluster from failure $ .kube % kubectl get pods --kubeconfig calico-1 -n strimzi -w NAME READY STATUS RESTARTS AGE my-cluster-broker-0 0 /1 Running 1 ( 2m6s ago ) 24h my-cluster-broker-1 0 /1 Running 1 ( 2m9s ago ) 23h my-cluster-broker-2 0 /1 Running 1 ( 2m9s ago ) 24h my-cluster-controller-3 1 /1 Running 1 ( 2m9s ago ) 24h my-cluster-controller-4 1 /1 Running 1 ( 2m6s ago ) 24h my-cluster-controller-5 1 /1 Running 1 ( 2m9s ago ) 24h strimzi-cluster-operator-5b7b9d9bf6-w4hxl 0 /1 Running 1 ( 2m6s ago ) 24h strimzi-cluster-operator-5b7b9d9bf6-w4hxl 1 /1 Running 1 ( 2m11s ago ) 24h my-cluster-broker-0 1 /1 Running 1 ( 2m12s ago ) 24h my-cluster-broker-2 1 /1 Running 1 ( 2m17s ago ) 24h my-cluster-broker-1 1 /1 Running 1 ( 2m56s ago ) 23h Testing if the ISR's come back from the central clusters The brokers from the recovered cluster are getting in sync with the partitions and is being reflected in the ISRs [ kafka@my-cluster-stretch2-broker-12 kafka ] $ bin/kafka-topics.sh --describe --bootstrap-server my-cluster-kafka-bootstrap.strimzi.svc:9092 --topic failover-test Topic: failover-test TopicId: 7U-yMkfgT1GfJRY-DoyEhQ PartitionCount: 6 ReplicationFactor: 6 Configs: min.insync.replicas = 2 Topic: failover-test Partition: 0 Leader: 8 Replicas: 8 ,12,13,14,0,1 Isr: 0 ,14,13,12,8 Elr: LastKnownElr: Topic: failover-test Partition: 1 Leader: 12 Replicas: 12 ,13,14,0,1,2 Isr: 0 ,14,13,2,12 Elr: LastKnownElr: Topic: failover-test Partition: 2 Leader: 14 Replicas: 13 ,14,0,1,2,6 Isr: 0 ,14,6,13,2 Elr: LastKnownElr: Topic: failover-test Partition: 3 Leader: 14 Replicas: 14 ,0,1,2,6,7 Isr: 0 ,14,6,2,7 Elr: LastKnownElr: Topic: failover-test Partition: 4 Leader: 6 Replicas: 0 ,1,2,6,7,8 Isr: 0 ,6,2,7,8 Elr: LastKnownElr: Topic: failover-test Partition: 5 Leader: 6 Replicas: 1 ,2,6,7,8,12 Isr: 6 ,2,12,7,8 Elr: LastKnownElr: Interpretation of Results The test confirms that Kafka's metadata quorum can successfully transition leadership to available controllers when the central cluster fails. Message production and consumption remain uninterrupted, demonstrating the resilience of stretched clusters. Upon recovery, the central cluster resumes operations, reinstating previous leader assignments where possible. The system effectively handles failover and restoration, ensuring high availability and minimal downtime. Conclusion This failover test validates the effectiveness of stretched Kafka clusters in handling central cluster failures while maintaining data integrity.","title":"Cluster Failover Testing"},{"location":"Testing-cluster-failover/#cluster-failover-testing","text":"","title":"Cluster Failover Testing"},{"location":"Testing-cluster-failover/#objective","text":"This test simulates a scenario in which the central cluster goes down. It evaluates the behavior of Kafka topics, leader election, in-sync replicas (ISRs), and how a stretched Kafka cluster handles failover. After testing the failover, the central cluster is recovered to determine if the old state is restored.","title":"Objective"},{"location":"Testing-cluster-failover/#cluster-setup","text":"We have three Kubernetes clusters with a stretched Kafka deployment across them.","title":"Cluster setup"},{"location":"Testing-cluster-failover/#cluster-configurations","text":"","title":"Cluster Configurations"},{"location":"Testing-cluster-failover/#central-cluster-calico-1","text":"$ .kube % kubectl get pods --kubeconfig calico-1 -n strimzi NAME READY STATUS RESTARTS AGE my-cluster-broker-0 1 /1 Running 0 23h my-cluster-broker-1 1 /1 Running 0 23h my-cluster-broker-2 1 /1 Running 0 23h my-cluster-controller-3 1 /1 Running 0 23h my-cluster-controller-4 1 /1 Running 0 23h my-cluster-controller-5 1 /1 Running 0 23h strimzi-cluster-operator-5b7b9d9bf6-w4hxl 1 /1 Running 0 23h","title":"Central Cluster (calico-1)"},{"location":"Testing-cluster-failover/#stretch-cluster-1-calico-2","text":"$ .kube % kubectl get pods --kubeconfig calico-2 -n strimzi NAME READY STATUS RESTARTS AGE my-cluster-stretch1-broker-6 1 /1 Running 0 23h my-cluster-stretch1-broker-7 1 /1 Running 0 23h my-cluster-stretch1-broker-8 1 /1 Running 0 23h my-cluster-stretch1-controller-10 1 /1 Running 0 23h my-cluster-stretch1-controller-11 1 /1 Running 0 23h my-cluster-stretch1-controller-9 1 /1 Running 0 23h strimzi-cluster-operator-6d7db9dd95-k5m24 1 /1 Running 0 23h","title":"Stretch Cluster 1 (calico-2)"},{"location":"Testing-cluster-failover/#stretch-cluster-2-calico-3","text":"$ .kube % kubectl get pods --kubeconfig calico-3 -n strimzi NAME READY STATUS RESTARTS AGE my-cluster-stretch2-broker-12 1 /1 Running 0 23h my-cluster-stretch2-broker-13 1 /1 Running 0 23h my-cluster-stretch2-broker-14 1 /1 Running 0 23h my-cluster-stretch2-controller-15 1 /1 Running 0 23h my-cluster-stretch2-controller-16 1 /1 Running 0 23h my-cluster-stretch2-controller-17 1 /1 Running 0 23h strimzi-cluster-operator-7966fb9659-zqfmv 1 /1 Running 0 23h The central cluster contains the Kafka and KafkaNodePool CRs: $ .kube % kubectl get kafka -n strimzi --kubeconfig calico-1 NAME DESIRED KAFKA REPLICAS DESIRED ZK REPLICAS READY METADATA STATE WARNINGS my-cluster $ .kube % kubectl get kafkanodepool -n strimzi --kubeconfig calico-1 NAME DESIRED REPLICAS ROLES NODEIDS broker 3 [ \"broker\" ] [ 0 ,1,2 ] controller 3 [ \"controller\" ] [ 3 ,4,5 ] stretch1-broker 3 [ \"broker\" ] [ 6 ,7,8 ] stretch1-controller 3 [ \"controller\" ] [ 9 ,10,11 ] stretch2-broker 3 [ \"broker\" ] [ 12 ,13,14 ] stretch2-controller 3 [ \"controller\" ] [ 15 ,16,17 ]","title":"Stretch Cluster 2 (calico-3)"},{"location":"Testing-cluster-failover/#listing-the-metadata-quorum","text":"Checking if the metadata reflects all brokers and controllers across clusters: [ kafka@my-cluster-broker-0 kafka ] $ bin/kafka-metadata-quorum.sh --bootstrap-server my-cluster-kafka-bootstrap.strimzi.svc:9092 describe --status ClusterId: 1RYWwDxMT8mT0lpiqtc69w LeaderId: 11 LeaderEpoch: 195814 HighWatermark: 168406 MaxFollowerLag: 0 MaxFollowerLagTimeMs: 54 CurrentVoters: [ 16 ,17,3,4,5,9,10,11,15 ] CurrentObservers: [ 0 ,1,2,6,7,8,12,13,14 ]","title":"Listing the metadata quorum"},{"location":"Testing-cluster-failover/#topic-creation-and-message-testing","text":"[ kafka@my-cluster-broker-0 kafka ] $ bin/kafka-topics.sh --create --bootstrap-server my-cluster-kafka-bootstrap.strimzi.svc:9092 --replication-factor 6 --partitions 6 --topic failover-test Created topic failover-test. Describing the topics shows that partition 4 and 5 have leaders from the central cluster and partition's all ISRs contains all brokers from the central cluster. [ kafka@my-cluster-broker-0 kafka ] $ bin/kafka-topics.sh --describe --bootstrap-server my-cluster-kafka-bootstrap.strimzi.svc:9092 --topic failover-test Topic: failover-test TopicId: 7U-yMkfgT1GfJRY-DoyEhQ PartitionCount: 6 ReplicationFactor: 6 Configs: min.insync.replicas = 2 Topic: failover-test Partition: 0 Leader: 8 Replicas: 8 ,12,13,14,0,1 Isr: 8 ,12,13,14,0,1 Elr: LastKnownElr: Topic: failover-test Partition: 1 Leader: 12 Replicas: 12 ,13,14,0,1,2 Isr: 12 ,13,14,0,1,2 Elr: LastKnownElr: Topic: failover-test Partition: 2 Leader: 13 Replicas: 13 ,14,0,1,2,6 Isr: 13 ,14,0,1,2,6 Elr: LastKnownElr: Topic: failover-test Partition: 3 Leader: 14 Replicas: 14 ,0,1,2,6,7 Isr: 14 ,0,1,2,6,7 Elr: LastKnownElr: Topic: failover-test Partition: 4 Leader: 0 Replicas: 0 ,1,2,6,7,8 Isr: 0 ,1,2,6,7,8 Elr: LastKnownElr: Topic: failover-test Partition: 5 Leader: 1 Replicas: 1 ,2,6,7,8,12 Isr: 1 ,2,6,7,8,12 Elr: LastKnownElr:","title":"Topic Creation and Message Testing"},{"location":"Testing-cluster-failover/#producing-and-consuming-messages-from-the-topic","text":"[ kafka@my-cluster-broker-0 kafka ] $ bin/kafka-console-producer.sh --bootstrap-server my-cluster-kafka-bootstrap.strimzi.svc:9092 --topic failover-test >asdfasdf >this is stretch >sending data from one cluster to the other >Hello Kafka >pushing enough messages to kafka cluster >Hello world >Testing >Testing asdf [ kafka@my-cluster-stretch1-broker-8 kafka ] $ bin/kafka-console-consumer.sh --bootstrap-server my-cluster-kafka-bootstrap.strimzi.svc:9092 --topic failover-test asdfasdf this is stretch sending data from one cluster to the other Hello Kafka pushing enough messages to kafka cluster Hello world Testing Testing asdf ^CProcessed a total of 8 messages","title":"Producing and consuming messages from the topic"},{"location":"Testing-cluster-failover/#simulating-central-cluster-failure","text":"Manually shutting down the central cluster to simulate a cluster failure $ .kube % kubectl get pods --kubeconfig calico-1 -n strimzi -v = 8 I0313 13 :57:19.830013 14803 loader.go:395 ] Config loaded from file: calico-1 I0313 13 :57:19.834745 14803 round_trippers.go:463 ] GET https://9.46.88.97:6443/api/v1/namespaces/strimzi/pods?limit = 500 I0313 13 :57:19.834762 14803 round_trippers.go:469 ] Request Headers: I0313 13 :57:19.834768 14803 round_trippers.go:473 ] Accept: application/json ; as = Table ; v = v1 ; g = meta.k8s.io,application/json ; as = Table ; v = v1beta1 ; g = meta.k8s.io,application/json I0313 13 :57:19.834771 14803 round_trippers.go:473 ] User-Agent: kubectl1.31.1/v1.31.1 ( darwin/arm64 ) kubernetes/948afe5 I0313 13 :57:49.837331 14803 round_trippers.go:574 ] Response Status: in 30002 milliseconds I0313 13 :57:49.837405 14803 round_trippers.go:577 ] Response Headers: I0313 13 :57:49.838573 14803 helpers.go:264 ] Connection error: Get https://9.46.88.97:6443/api/v1/namespaces/strimzi/pods?limit = 500 : dial tcp 9 .46.88.97:6443: i/o timeout Unable to connect to the server: dial tcp 9 .46.88.97:6443: i/o timeout","title":"Simulating Central Cluster Failure"},{"location":"Testing-cluster-failover/#testing-if-we-can-produce-and-consume-with-the-other-two-clusters","text":"We're using cluster 2 and 3 to produce and consume [ kafka@my-cluster-stretch2-broker-12 kafka ] $ bin/kafka-console-producer.sh --bootstrap-server my-cluster-kafka-bootstrap.strimzi.svc:9092 --topic failover-test >hello world >Seems like the data is not lost [ kafka@my-cluster-stretch1-broker-8 kafka ] $ bin/kafka-console-consumer.sh --bootstrap-server my-cluster-kafka-bootstrap.strimzi.svc:9092 --topic failover-test --from-beginning asdfasdf this is stretch sending data from one cluster to the other Hello Kafka pushing enough messages to kafka cluster Hello world Testing Testing asdf hello world Seems like the data is not lost ^CProcessed a total of 10 messages Produce and consume works fine and it seems like the previously sent messages are being retrieved properly.","title":"Testing if we can produce and consume with the other two clusters"},{"location":"Testing-cluster-failover/#checking-if-leader-election-elected-new-partition-leaders","text":"Partition 4 and 5 which had leaders from the failed central cluster are assigned new leaders from available cluster (cluster 2). [ kafka@my-cluster-stretch2-broker-12 kafka ] $ bin/kafka-topics.sh --describe --bootstrap-server my-cluster-kafka-bootstrap.strimzi.svc:9092 --topic failover-test Topic: failover-test TopicId: 7U-yMkfgT1GfJRY-DoyEhQ PartitionCount: 6 ReplicationFactor: 6 Configs: min.insync.replicas = 2 Topic: failover-test Partition: 0 Leader: 8 Replicas: 8 ,12,13,14,0,1 Isr: 8 ,12,14,13 Elr: LastKnownElr: Topic: failover-test Partition: 1 Leader: 12 Replicas: 12 ,13,14,0,1,2 Isr: 12 ,14,13 Elr: LastKnownElr: Topic: failover-test Partition: 2 Leader: 13 Replicas: 13 ,14,0,1,2,6 Isr: 14 ,6,13 Elr: LastKnownElr: Topic: failover-test Partition: 3 Leader: 14 Replicas: 14 ,0,1,2,6,7 Isr: 14 ,6,7 Elr: LastKnownElr: Topic: failover-test Partition: 4 Leader: 6 Replicas: 0 ,1,2,6,7,8 Isr: 6 ,7,8 Elr: LastKnownElr: Topic: failover-test Partition: 5 Leader: 6 Replicas: 1 ,2,6,7,8,12 Isr: 6 ,7,8,12 Elr: LastKnownElr:","title":"Checking if leader election elected new partition leaders"},{"location":"Testing-cluster-failover/#recovering-the-central-cluster","text":"Manually recovering the central cluster from failure $ .kube % kubectl get pods --kubeconfig calico-1 -n strimzi -w NAME READY STATUS RESTARTS AGE my-cluster-broker-0 0 /1 Running 1 ( 2m6s ago ) 24h my-cluster-broker-1 0 /1 Running 1 ( 2m9s ago ) 23h my-cluster-broker-2 0 /1 Running 1 ( 2m9s ago ) 24h my-cluster-controller-3 1 /1 Running 1 ( 2m9s ago ) 24h my-cluster-controller-4 1 /1 Running 1 ( 2m6s ago ) 24h my-cluster-controller-5 1 /1 Running 1 ( 2m9s ago ) 24h strimzi-cluster-operator-5b7b9d9bf6-w4hxl 0 /1 Running 1 ( 2m6s ago ) 24h strimzi-cluster-operator-5b7b9d9bf6-w4hxl 1 /1 Running 1 ( 2m11s ago ) 24h my-cluster-broker-0 1 /1 Running 1 ( 2m12s ago ) 24h my-cluster-broker-2 1 /1 Running 1 ( 2m17s ago ) 24h my-cluster-broker-1 1 /1 Running 1 ( 2m56s ago ) 23h","title":"Recovering the central cluster"},{"location":"Testing-cluster-failover/#testing-if-the-isrs-come-back-from-the-central-clusters","text":"The brokers from the recovered cluster are getting in sync with the partitions and is being reflected in the ISRs [ kafka@my-cluster-stretch2-broker-12 kafka ] $ bin/kafka-topics.sh --describe --bootstrap-server my-cluster-kafka-bootstrap.strimzi.svc:9092 --topic failover-test Topic: failover-test TopicId: 7U-yMkfgT1GfJRY-DoyEhQ PartitionCount: 6 ReplicationFactor: 6 Configs: min.insync.replicas = 2 Topic: failover-test Partition: 0 Leader: 8 Replicas: 8 ,12,13,14,0,1 Isr: 0 ,14,13,12,8 Elr: LastKnownElr: Topic: failover-test Partition: 1 Leader: 12 Replicas: 12 ,13,14,0,1,2 Isr: 0 ,14,13,2,12 Elr: LastKnownElr: Topic: failover-test Partition: 2 Leader: 14 Replicas: 13 ,14,0,1,2,6 Isr: 0 ,14,6,13,2 Elr: LastKnownElr: Topic: failover-test Partition: 3 Leader: 14 Replicas: 14 ,0,1,2,6,7 Isr: 0 ,14,6,2,7 Elr: LastKnownElr: Topic: failover-test Partition: 4 Leader: 6 Replicas: 0 ,1,2,6,7,8 Isr: 0 ,6,2,7,8 Elr: LastKnownElr: Topic: failover-test Partition: 5 Leader: 6 Replicas: 1 ,2,6,7,8,12 Isr: 6 ,2,12,7,8 Elr: LastKnownElr:","title":"Testing if the ISR's come back from the central clusters"},{"location":"Testing-cluster-failover/#interpretation-of-results","text":"The test confirms that Kafka's metadata quorum can successfully transition leadership to available controllers when the central cluster fails. Message production and consumption remain uninterrupted, demonstrating the resilience of stretched clusters. Upon recovery, the central cluster resumes operations, reinstating previous leader assignments where possible. The system effectively handles failover and restoration, ensuring high availability and minimal downtime.","title":"Interpretation of Results"},{"location":"Testing-cluster-failover/#conclusion","text":"This failover test validates the effectiveness of stretched Kafka clusters in handling central cluster failures while maintaining data integrity.","title":"Conclusion"},{"location":"Testing-failover-and-resiliency/","text":"Failover Testing This document provides a detailed guide on testing failover scenarios, leader election, and topic availability in a multi-cluster Kafka deployment. Cluster Setup The Kafka clusters are deployed across multiple Kubernetes clusters, as shown below $ kubectl get pods -n real --kubeconfig config-test-a NAME READY STATUS RESTARTS AGE my-cluster-broker-0 1 /1 Running 0 3h56m my-cluster-broker-1 1 /1 Running 0 3h56m my-cluster-broker-2 1 /1 Running 0 3h56m my-cluster-controller-3 1 /1 Running 0 3h55m my-cluster-controller-4 1 /1 Running 0 3h55m my-cluster-controller-5 1 /1 Running 0 3h55m strimzi-cluster-operator-v0.44.0-6ff79bb868-vws4n 1 /1 Running 0 4h20m $ kubectl get pods -n real --kubeconfig config-test-b NAME READY STATUS RESTARTS AGE my-cluster-stretch1-broker-6 1 /1 Running 0 3h56m my-cluster-stretch1-broker-7 1 /1 Running 0 3h56m my-cluster-stretch1-broker-8 1 /1 Running 0 3h56m my-cluster-stretch1-controller-10 1 /1 Running 0 3h56m my-cluster-stretch1-controller-11 1 /1 Running 0 3h56m my-cluster-stretch1-controller-9 1 /1 Running 0 3h56m strimzi-cluster-operator-v0.44.0-78979ffb66-hkjf6 1 /1 Running 0 2d17h $ kubectl get pods -n real --kubeconfig config-test-c NAME READY STATUS RESTARTS AGE my-cluster-stretch2-broker-12 1 /1 Running 0 3h56m my-cluster-stretch2-broker-13 1 /1 Running 0 3h56m my-cluster-stretch2-broker-14 1 /1 Running 0 3h56m my-cluster-stretch2-controller-15 1 /1 Running 0 3h56m my-cluster-stretch2-controller-16 1 /1 Running 0 3h56m my-cluster-stretch2-controller-17 1 /1 Running 0 3h56m strimzi-cluster-operator-v0.44.0-6c4f95869b-2qgjg 1 /1 Running 0 2d17h Topic availability and leader election testing Create a topic with all brokers participating sh-5.1$ ./kafka-topics.sh --create --bootstrap-server my-cluster-kafka-bootstrap:9092 --replication-factor 9 --partitions 3 --topic long-replicated-topic Created topic long-replicated-topic. Describe topic sh-5.1$ ./kafka-topics.sh --describe --bootstrap-server my-cluster-kafka-bootstrap:9092 --topic long-replicated-topic Topic: long-replicated-topic TopicId: 0qzO6dCFRiy4n3aVgPZ-gA PartitionCount: 3 ReplicationFactor: 9 Configs: min.insync.replicas = 2 Topic: long-replicated-topic Partition: 0 Leader: 8 Replicas: 8 ,12,13,14,0,1,2,6,7 Isr: 8 ,12,13,14,0,1,2,6,7 Elr: LastKnownElr: Topic: long-replicated-topic Partition: 1 Leader: 12 Replicas: 12 ,13,14,0,1,2,6,7,8 Isr: 12 ,13,14,0,1,2,6,7,8 Elr: LastKnownElr: Topic: long-replicated-topic Partition: 2 Leader: 13 Replicas: 13 ,14,0,1,2,6,7,8,12 Isr: 13 ,14,0,1,2,6,7,8,12 Elr: LastKnownElr: Partition 0, 1 and 2 have leaders as 8, 12 and 13 respectively. We're going to delete some of these leader brokers to see if new partition leaders are elected to another available broker. Deleting broker 8 to see leadership transition $ kubectl delete pod my-cluster-stretch1-broker-8 -n real --kubeconfig config-test-b pod \"my-cluster-stretch1-broker-8\" deleted sh-5.1$ ./kafka-topics.sh --describe --bootstrap-server my-cluster-kafka-bootstrap:9092 --topic long-replicated-topic Topic: long-replicated-topic TopicId: 0qzO6dCFRiy4n3aVgPZ-gA PartitionCount: 3 ReplicationFactor: 9 Configs: min.insync.replicas = 2 Topic: long-replicated-topic Partition: 0 Leader: 12 Replicas: 8 ,12,13,14,0,1,2,6,7 Isr: 12 ,13,14,0,1,2,6,7 Elr: LastKnownElr: Topic: long-replicated-topic Partition: 1 Leader: 12 Replicas: 12 ,13,14,0,1,2,6,7,8 Isr: 12 ,13,14,0,1,2,6,7 Elr: LastKnownElr: Topic: long-replicated-topic Partition: 2 Leader: 13 Replicas: 13 ,14,0,1,2,6,7,8,12 Isr: 13 ,14,0,1,2,6,7,12 Elr: LastKnownElr: We can see that leadership has been transitioned to 12 Deleting broker 12 to see leadership transition $ kubectl delete pod my-cluster-stretch2-broker-12 -n real --kubeconfig config-test-c pod \"my-cluster-stretch2-broker-12\" deleted sh-5.1$ ./kafka-topics.sh --describe --bootstrap-server my-cluster-kafka-bootstrap:9092 --topic long-replicated-topic Topic: long-replicated-topic TopicId: 0qzO6dCFRiy4n3aVgPZ-gA PartitionCount: 3 ReplicationFactor: 9 Configs: min.insync.replicas = 2 Topic: long-replicated-topic Partition: 0 Leader: 8 Replicas: 8 ,12,13,14,0,1,2,6,7 Isr: 0 ,14,1,6,13,2,7,8 Elr: LastKnownElr: Topic: long-replicated-topic Partition: 1 Leader: 13 Replicas: 12 ,13,14,0,1,2,6,7,8 Isr: 0 ,14,1,6,13,2,7,8 Elr: LastKnownElr: Topic: long-replicated-topic Partition: 2 Leader: 13 Replicas: 13 ,14,0,1,2,6,7,8,12 Isr: 0 ,14,1,6,13,2,7,8 Elr: LastKnownElr: Conclusion: The topic remains stable and leader election happens normally even after the leader has been deleted Controller leader election test The controllers across stretch cluster must have an active leader. If a leader controller goes down, any available controller from any cluster should be elected as leader. Checking Initial Leader sh-5.1$ ./kafka-metadata-quorum.sh --bootstrap-server my-cluster-kafka-bootstrap:9092 describe --status ClusterId: 6LKJ0UFeRwKeyU_G1HdZxg LeaderId: 5 LeaderEpoch: 116 HighWatermark: 30543 MaxFollowerLag: 0 MaxFollowerLagTimeMs: 295 CurrentVoters: [ 16 ,17,3,4,5,9,10,11,15 ] CurrentObservers: [ 0 ,1,2,6,7,8,12,13,14 ] The current leader is controller 5. If controller 5 goes down, a new controller must be elected as leader. Deleting Controller 5 $ kubectl delete pod my-cluster-controller-5 -n real --kubeconfig config-test-a pod \"my-cluster-controller-5\" deleted New Leader Election Output sh-5.1$ ./kafka-metadata-quorum.sh --bootstrap-server my-cluster-kafka-bootstrap:9092 describe --status ClusterId: 6LKJ0UFeRwKeyU_G1HdZxg LeaderId: 4 LeaderEpoch: 118 HighWatermark: 30960 MaxFollowerLag: 30961 MaxFollowerLagTimeMs: -1 CurrentVoters: [ 16 ,17,3,4,5,9,10,11,15 ] CurrentObservers: [ 0 ,1,2,6,7,8,12,13,14 ] The new leader is controller 4 Deleting Controller 4 $ kubectl delete pod my-cluster-controller-4 -n real --kubeconfig config-test-a pod \"my-cluster-controller-4\" deleted New Leader Election Output sh-5.1$ ./kafka-metadata-quorum.sh --bootstrap-server my-cluster-kafka-bootstrap:9092 describe --status ClusterId: 6LKJ0UFeRwKeyU_G1HdZxg LeaderId: 3 LeaderEpoch: 120 HighWatermark: 31046 MaxFollowerLag: 31047 MaxFollowerLagTimeMs: -1 CurrentVoters: [ 16 ,17,3,4,5,9,10,11,15 ] CurrentObservers: [ 0 ,1,2,6,7,8,12,13,14 ] These results however shows leader election works inside a cluster only, hence we deleted even more leaders to see if leader election works across cluster. Checking cross cluster leader election Current leader is 5 sh-5.1$ ./kafka-metadata-quorum.sh --bootstrap-server my-cluster-kafka-bootstrap:9092 describe --status ClusterId: 6LKJ0UFeRwKeyU_G1HdZxg LeaderId: 5 LeaderEpoch: 122 HighWatermark: 31124 MaxFollowerLag: 31125 MaxFollowerLagTimeMs: -1 CurrentVoters: [ 16 ,17,3,4,5,9,10,11,15 ] CurrentObservers: [ 0 ,1,2,6,7,8,12,13,14 ] Deleting leader 5 $ kubectl delete pod my-cluster-controller-5 -n real --kubeconfig config-test-a pod \"my-cluster-controller-5\" deleted New leader 9 sh-5.1$ ./kafka-metadata-quorum.sh --bootstrap-server my-cluster-kafka-bootstrap:9092 describe --status ClusterId: 6LKJ0UFeRwKeyU_G1HdZxg LeaderId: 9 LeaderEpoch: 124 HighWatermark: 31186 MaxFollowerLag: 0 MaxFollowerLagTimeMs: 271 CurrentVoters: [ 16 ,17,3,4,5,9,10,11,15 ] CurrentObservers: [ 0 ,1,2,6,7,8,12,13,14 ] Conclusion: Cross cluster controller leader election works seamlessly in a stretch cluster setup","title":"Failover Testing"},{"location":"Testing-failover-and-resiliency/#failover-testing","text":"This document provides a detailed guide on testing failover scenarios, leader election, and topic availability in a multi-cluster Kafka deployment.","title":"Failover Testing"},{"location":"Testing-failover-and-resiliency/#cluster-setup","text":"The Kafka clusters are deployed across multiple Kubernetes clusters, as shown below $ kubectl get pods -n real --kubeconfig config-test-a NAME READY STATUS RESTARTS AGE my-cluster-broker-0 1 /1 Running 0 3h56m my-cluster-broker-1 1 /1 Running 0 3h56m my-cluster-broker-2 1 /1 Running 0 3h56m my-cluster-controller-3 1 /1 Running 0 3h55m my-cluster-controller-4 1 /1 Running 0 3h55m my-cluster-controller-5 1 /1 Running 0 3h55m strimzi-cluster-operator-v0.44.0-6ff79bb868-vws4n 1 /1 Running 0 4h20m $ kubectl get pods -n real --kubeconfig config-test-b NAME READY STATUS RESTARTS AGE my-cluster-stretch1-broker-6 1 /1 Running 0 3h56m my-cluster-stretch1-broker-7 1 /1 Running 0 3h56m my-cluster-stretch1-broker-8 1 /1 Running 0 3h56m my-cluster-stretch1-controller-10 1 /1 Running 0 3h56m my-cluster-stretch1-controller-11 1 /1 Running 0 3h56m my-cluster-stretch1-controller-9 1 /1 Running 0 3h56m strimzi-cluster-operator-v0.44.0-78979ffb66-hkjf6 1 /1 Running 0 2d17h $ kubectl get pods -n real --kubeconfig config-test-c NAME READY STATUS RESTARTS AGE my-cluster-stretch2-broker-12 1 /1 Running 0 3h56m my-cluster-stretch2-broker-13 1 /1 Running 0 3h56m my-cluster-stretch2-broker-14 1 /1 Running 0 3h56m my-cluster-stretch2-controller-15 1 /1 Running 0 3h56m my-cluster-stretch2-controller-16 1 /1 Running 0 3h56m my-cluster-stretch2-controller-17 1 /1 Running 0 3h56m strimzi-cluster-operator-v0.44.0-6c4f95869b-2qgjg 1 /1 Running 0 2d17h","title":"Cluster Setup"},{"location":"Testing-failover-and-resiliency/#topic-availability-and-leader-election-testing","text":"","title":"Topic availability and leader election testing"},{"location":"Testing-failover-and-resiliency/#create-a-topic-with-all-brokers-participating","text":"sh-5.1$ ./kafka-topics.sh --create --bootstrap-server my-cluster-kafka-bootstrap:9092 --replication-factor 9 --partitions 3 --topic long-replicated-topic Created topic long-replicated-topic.","title":"Create a topic with all brokers participating"},{"location":"Testing-failover-and-resiliency/#describe-topic","text":"sh-5.1$ ./kafka-topics.sh --describe --bootstrap-server my-cluster-kafka-bootstrap:9092 --topic long-replicated-topic Topic: long-replicated-topic TopicId: 0qzO6dCFRiy4n3aVgPZ-gA PartitionCount: 3 ReplicationFactor: 9 Configs: min.insync.replicas = 2 Topic: long-replicated-topic Partition: 0 Leader: 8 Replicas: 8 ,12,13,14,0,1,2,6,7 Isr: 8 ,12,13,14,0,1,2,6,7 Elr: LastKnownElr: Topic: long-replicated-topic Partition: 1 Leader: 12 Replicas: 12 ,13,14,0,1,2,6,7,8 Isr: 12 ,13,14,0,1,2,6,7,8 Elr: LastKnownElr: Topic: long-replicated-topic Partition: 2 Leader: 13 Replicas: 13 ,14,0,1,2,6,7,8,12 Isr: 13 ,14,0,1,2,6,7,8,12 Elr: LastKnownElr: Partition 0, 1 and 2 have leaders as 8, 12 and 13 respectively. We're going to delete some of these leader brokers to see if new partition leaders are elected to another available broker.","title":"Describe topic"},{"location":"Testing-failover-and-resiliency/#deleting-broker-8-to-see-leadership-transition","text":"$ kubectl delete pod my-cluster-stretch1-broker-8 -n real --kubeconfig config-test-b pod \"my-cluster-stretch1-broker-8\" deleted sh-5.1$ ./kafka-topics.sh --describe --bootstrap-server my-cluster-kafka-bootstrap:9092 --topic long-replicated-topic Topic: long-replicated-topic TopicId: 0qzO6dCFRiy4n3aVgPZ-gA PartitionCount: 3 ReplicationFactor: 9 Configs: min.insync.replicas = 2 Topic: long-replicated-topic Partition: 0 Leader: 12 Replicas: 8 ,12,13,14,0,1,2,6,7 Isr: 12 ,13,14,0,1,2,6,7 Elr: LastKnownElr: Topic: long-replicated-topic Partition: 1 Leader: 12 Replicas: 12 ,13,14,0,1,2,6,7,8 Isr: 12 ,13,14,0,1,2,6,7 Elr: LastKnownElr: Topic: long-replicated-topic Partition: 2 Leader: 13 Replicas: 13 ,14,0,1,2,6,7,8,12 Isr: 13 ,14,0,1,2,6,7,12 Elr: LastKnownElr: We can see that leadership has been transitioned to 12","title":"Deleting broker 8 to see leadership transition"},{"location":"Testing-failover-and-resiliency/#deleting-broker-12-to-see-leadership-transition","text":"$ kubectl delete pod my-cluster-stretch2-broker-12 -n real --kubeconfig config-test-c pod \"my-cluster-stretch2-broker-12\" deleted sh-5.1$ ./kafka-topics.sh --describe --bootstrap-server my-cluster-kafka-bootstrap:9092 --topic long-replicated-topic Topic: long-replicated-topic TopicId: 0qzO6dCFRiy4n3aVgPZ-gA PartitionCount: 3 ReplicationFactor: 9 Configs: min.insync.replicas = 2 Topic: long-replicated-topic Partition: 0 Leader: 8 Replicas: 8 ,12,13,14,0,1,2,6,7 Isr: 0 ,14,1,6,13,2,7,8 Elr: LastKnownElr: Topic: long-replicated-topic Partition: 1 Leader: 13 Replicas: 12 ,13,14,0,1,2,6,7,8 Isr: 0 ,14,1,6,13,2,7,8 Elr: LastKnownElr: Topic: long-replicated-topic Partition: 2 Leader: 13 Replicas: 13 ,14,0,1,2,6,7,8,12 Isr: 0 ,14,1,6,13,2,7,8 Elr: LastKnownElr: Conclusion: The topic remains stable and leader election happens normally even after the leader has been deleted","title":"Deleting broker 12 to see leadership transition"},{"location":"Testing-failover-and-resiliency/#controller-leader-election-test","text":"The controllers across stretch cluster must have an active leader. If a leader controller goes down, any available controller from any cluster should be elected as leader.","title":"Controller leader election test"},{"location":"Testing-failover-and-resiliency/#checking-initial-leader","text":"sh-5.1$ ./kafka-metadata-quorum.sh --bootstrap-server my-cluster-kafka-bootstrap:9092 describe --status ClusterId: 6LKJ0UFeRwKeyU_G1HdZxg LeaderId: 5 LeaderEpoch: 116 HighWatermark: 30543 MaxFollowerLag: 0 MaxFollowerLagTimeMs: 295 CurrentVoters: [ 16 ,17,3,4,5,9,10,11,15 ] CurrentObservers: [ 0 ,1,2,6,7,8,12,13,14 ] The current leader is controller 5. If controller 5 goes down, a new controller must be elected as leader.","title":"Checking Initial Leader"},{"location":"Testing-failover-and-resiliency/#deleting-controller-5","text":"$ kubectl delete pod my-cluster-controller-5 -n real --kubeconfig config-test-a pod \"my-cluster-controller-5\" deleted","title":"Deleting Controller 5"},{"location":"Testing-failover-and-resiliency/#new-leader-election-output","text":"sh-5.1$ ./kafka-metadata-quorum.sh --bootstrap-server my-cluster-kafka-bootstrap:9092 describe --status ClusterId: 6LKJ0UFeRwKeyU_G1HdZxg LeaderId: 4 LeaderEpoch: 118 HighWatermark: 30960 MaxFollowerLag: 30961 MaxFollowerLagTimeMs: -1 CurrentVoters: [ 16 ,17,3,4,5,9,10,11,15 ] CurrentObservers: [ 0 ,1,2,6,7,8,12,13,14 ] The new leader is controller 4","title":"New Leader Election Output"},{"location":"Testing-failover-and-resiliency/#deleting-controller-4","text":"$ kubectl delete pod my-cluster-controller-4 -n real --kubeconfig config-test-a pod \"my-cluster-controller-4\" deleted","title":"Deleting Controller 4"},{"location":"Testing-failover-and-resiliency/#new-leader-election-output_1","text":"sh-5.1$ ./kafka-metadata-quorum.sh --bootstrap-server my-cluster-kafka-bootstrap:9092 describe --status ClusterId: 6LKJ0UFeRwKeyU_G1HdZxg LeaderId: 3 LeaderEpoch: 120 HighWatermark: 31046 MaxFollowerLag: 31047 MaxFollowerLagTimeMs: -1 CurrentVoters: [ 16 ,17,3,4,5,9,10,11,15 ] CurrentObservers: [ 0 ,1,2,6,7,8,12,13,14 ] These results however shows leader election works inside a cluster only, hence we deleted even more leaders to see if leader election works across cluster.","title":"New Leader Election Output"},{"location":"Testing-failover-and-resiliency/#checking-cross-cluster-leader-election","text":"Current leader is 5 sh-5.1$ ./kafka-metadata-quorum.sh --bootstrap-server my-cluster-kafka-bootstrap:9092 describe --status ClusterId: 6LKJ0UFeRwKeyU_G1HdZxg LeaderId: 5 LeaderEpoch: 122 HighWatermark: 31124 MaxFollowerLag: 31125 MaxFollowerLagTimeMs: -1 CurrentVoters: [ 16 ,17,3,4,5,9,10,11,15 ] CurrentObservers: [ 0 ,1,2,6,7,8,12,13,14 ] Deleting leader 5 $ kubectl delete pod my-cluster-controller-5 -n real --kubeconfig config-test-a pod \"my-cluster-controller-5\" deleted New leader 9 sh-5.1$ ./kafka-metadata-quorum.sh --bootstrap-server my-cluster-kafka-bootstrap:9092 describe --status ClusterId: 6LKJ0UFeRwKeyU_G1HdZxg LeaderId: 9 LeaderEpoch: 124 HighWatermark: 31186 MaxFollowerLag: 0 MaxFollowerLagTimeMs: 271 CurrentVoters: [ 16 ,17,3,4,5,9,10,11,15 ] CurrentObservers: [ 0 ,1,2,6,7,8,12,13,14 ] Conclusion: Cross cluster controller leader election works seamlessly in a stretch cluster setup","title":"Checking cross cluster leader election"},{"location":"Testing-performance/","text":"Performance Testing Open messaging benchmark framework The Open Messaging Benchmark framework was used to benchmark the stretch Kafka clusters. By following this tutorial , we obtained benchmark results for three configurations: Regular Kafka cluster Stretch cluster with Submariner Stretch cluster with Cilium The Open messaging benchmark framework deploys 8 benchmark workers and one driver in the K8s environment. Once the tests start, the driver employs the workers to produce/consume messages to the cluster using its bootstrap url. Configuration For the stretch cluster configurations, the OpenShift (OCP)/K8s clusters were located in the same data centers, minimizing network latency and focusing on the additional latency introduced by the networking tools in use. Regular Kafka cluster We used a regular Kraft Kafka cluster with three brokers, three controllers, and ephemeral storage. This Custom Resource (CR) was applied in an OCP environment. apiVersion : kafka.strimzi.io/v1beta2 kind : KafkaNodePool metadata : name : controller labels : strimzi.io/cluster : my-cluster spec : replicas : 3 roles : - controller storage : type : jbod volumes : - id : 0 type : ephemeral kraftMetadata : shared --- apiVersion : kafka.strimzi.io/v1beta2 kind : KafkaNodePool metadata : name : broker labels : strimzi.io/cluster : my-cluster spec : replicas : 3 roles : - broker storage : type : jbod volumes : - id : 0 type : ephemeral kraftMetadata : shared --- apiVersion : kafka.strimzi.io/v1beta2 kind : Kafka metadata : name : my-cluster annotations : strimzi.io/node-pools : enabled strimzi.io/kraft : enabled spec : kafka : version : 3.8.0 metadataVersion : 3.8-IV0 listeners : - name : plain port : 9092 type : internal tls : false - name : tls port : 9093 type : internal tls : true config : offsets.topic.replication.factor : 3 transaction.state.log.replication.factor : 3 transaction.state.log.min.isr : 2 default.replication.factor : 3 min.insync.replicas : 2 entityOperator : topicOperator : {} userOperator : {} Stretch cluster with Submariner The Kafka cluster was deployed across three OCP environments, each hosting three brokers and three controllers. As with the regular Kafka cluster, we used ephemeral storage. Submariner was deployed on all three clusters and connected using the subctl binary. The CR for this can be found under the Deploying Strimzi in Stretch Mode page. Stretched kafka cluster with Cilium Three K8s clusters were connected using the Cilium cli and then stretch cluster was created on this setup using the CR similar to that of Submariner with 3 brokers and 3 controllers. Findings Three test iterations were performed for each environment to eliminate environmental noise. The results were exported as JSON files and plotted using Matplotlib. The following four parameters were analyzed: Publish Delay (ms) End to end Delay (ms) Publish Rate (messages/sec) Consume Rate (messages/sec) The results were stored in directories named non-stretch, stretch-cilium, and stretch-submariner. The plotted results are shown below: Each color in the graph represents the three test iterations for each configuration Conclusion The results indicate that: The stretched Kafka cluster with Submariner introduces significant additional latency, making it less comparable to the regular Kafka cluster. The stretched Kafka cluster with Cilium provides performance levels comparable to the regular Kafka cluster.","title":"Performance Testing"},{"location":"Testing-performance/#performance-testing","text":"","title":"Performance Testing"},{"location":"Testing-performance/#open-messaging-benchmark-framework","text":"The Open Messaging Benchmark framework was used to benchmark the stretch Kafka clusters. By following this tutorial , we obtained benchmark results for three configurations: Regular Kafka cluster Stretch cluster with Submariner Stretch cluster with Cilium The Open messaging benchmark framework deploys 8 benchmark workers and one driver in the K8s environment. Once the tests start, the driver employs the workers to produce/consume messages to the cluster using its bootstrap url.","title":"Open messaging benchmark framework"},{"location":"Testing-performance/#configuration","text":"For the stretch cluster configurations, the OpenShift (OCP)/K8s clusters were located in the same data centers, minimizing network latency and focusing on the additional latency introduced by the networking tools in use.","title":"Configuration"},{"location":"Testing-performance/#regular-kafka-cluster","text":"We used a regular Kraft Kafka cluster with three brokers, three controllers, and ephemeral storage. This Custom Resource (CR) was applied in an OCP environment. apiVersion : kafka.strimzi.io/v1beta2 kind : KafkaNodePool metadata : name : controller labels : strimzi.io/cluster : my-cluster spec : replicas : 3 roles : - controller storage : type : jbod volumes : - id : 0 type : ephemeral kraftMetadata : shared --- apiVersion : kafka.strimzi.io/v1beta2 kind : KafkaNodePool metadata : name : broker labels : strimzi.io/cluster : my-cluster spec : replicas : 3 roles : - broker storage : type : jbod volumes : - id : 0 type : ephemeral kraftMetadata : shared --- apiVersion : kafka.strimzi.io/v1beta2 kind : Kafka metadata : name : my-cluster annotations : strimzi.io/node-pools : enabled strimzi.io/kraft : enabled spec : kafka : version : 3.8.0 metadataVersion : 3.8-IV0 listeners : - name : plain port : 9092 type : internal tls : false - name : tls port : 9093 type : internal tls : true config : offsets.topic.replication.factor : 3 transaction.state.log.replication.factor : 3 transaction.state.log.min.isr : 2 default.replication.factor : 3 min.insync.replicas : 2 entityOperator : topicOperator : {} userOperator : {}","title":"Regular Kafka cluster"},{"location":"Testing-performance/#stretch-cluster-with-submariner","text":"The Kafka cluster was deployed across three OCP environments, each hosting three brokers and three controllers. As with the regular Kafka cluster, we used ephemeral storage. Submariner was deployed on all three clusters and connected using the subctl binary. The CR for this can be found under the Deploying Strimzi in Stretch Mode page.","title":"Stretch cluster with Submariner"},{"location":"Testing-performance/#stretched-kafka-cluster-with-cilium","text":"Three K8s clusters were connected using the Cilium cli and then stretch cluster was created on this setup using the CR similar to that of Submariner with 3 brokers and 3 controllers.","title":"Stretched kafka cluster with Cilium"},{"location":"Testing-performance/#findings","text":"Three test iterations were performed for each environment to eliminate environmental noise. The results were exported as JSON files and plotted using Matplotlib. The following four parameters were analyzed: Publish Delay (ms) End to end Delay (ms) Publish Rate (messages/sec) Consume Rate (messages/sec) The results were stored in directories named non-stretch, stretch-cilium, and stretch-submariner. The plotted results are shown below: Each color in the graph represents the three test iterations for each configuration","title":"Findings"},{"location":"Testing-performance/#conclusion","text":"The results indicate that: The stretched Kafka cluster with Submariner introduces significant additional latency, making it less comparable to the regular Kafka cluster. The stretched Kafka cluster with Cilium provides performance levels comparable to the regular Kafka cluster.","title":"Conclusion"},{"location":"building-strimzi-for-stretch-cluster/","text":"Building Strimzi Kafka operator for stretch clusters The Strimzi community is actively transitioning to KRaft, leading to frequent changes in the main branch. At of the time of writing, Strimzi is in the process of removing ZooKeeper-related code. To ensure stability, we have based our development for stretch clusters on Strimzi 0.44.0 . The prototype code for stretch clusters is available in the following branch . This branch is derived from the Strimzi 0.44.0 release tag. To view the changes made on top of Strimzi 0.44.0 , refer to the following comparison link . Building the operator Clone the repository git clone https://github.com/aswinayyolath/strimzi-kafka-operator.git Checkout the stretch cluster branch git checkout strecth-cluster-prototype Set Docker registry variables Before building the operator, ensure that the DOCKER_ORG and DOCKER_REGISTRY environment variables are set correctly. These should match your Docker Hub username and the Docker registry you are using. If DOCKER_REGISTRY is unset, it defaults to docker.io . For more details, refer to the Strimzi developer guide . export DOCKER_ORG = <your_docker_hub_username> export DOCKER_REGISTRY = <your_docker_registry> # Defaults to docker.io if unset Build the operator Since this is a prototype, we have not implemented test coverage yet. To avoid test compilation errors during the build process, disable test execution: DOCKER_PLATFORM = \"--platform linux/amd64\" make MVN_ARGS = '-Dmaven.test.skip=true' all Note \ud83d\udea8 If you encounter an error related to unused declared dependencies, such as Unused declared dependencies found , you might need to comment out the dependencies mentioned in the error message within the corresponding pom.xml file. \ud83d\udea8 Testing the operator image without building If you prefer to test the operator without building it from source, you can use the pre-built image available on Docker Hub : aswinayyolath/stretchcluster:latest A detailed guide on testing the stretch cluster will be provided in a later section.","title":"Building Strimzi Kafka operator for stretch clusters"},{"location":"building-strimzi-for-stretch-cluster/#building-strimzi-kafka-operator-for-stretch-clusters","text":"The Strimzi community is actively transitioning to KRaft, leading to frequent changes in the main branch. At of the time of writing, Strimzi is in the process of removing ZooKeeper-related code. To ensure stability, we have based our development for stretch clusters on Strimzi 0.44.0 . The prototype code for stretch clusters is available in the following branch . This branch is derived from the Strimzi 0.44.0 release tag. To view the changes made on top of Strimzi 0.44.0 , refer to the following comparison link .","title":"Building Strimzi Kafka operator for stretch clusters"},{"location":"building-strimzi-for-stretch-cluster/#building-the-operator","text":"","title":"Building the operator"},{"location":"building-strimzi-for-stretch-cluster/#clone-the-repository","text":"git clone https://github.com/aswinayyolath/strimzi-kafka-operator.git","title":"Clone the repository"},{"location":"building-strimzi-for-stretch-cluster/#checkout-the-stretch-cluster-branch","text":"git checkout strecth-cluster-prototype","title":"Checkout the stretch cluster branch"},{"location":"building-strimzi-for-stretch-cluster/#set-docker-registry-variables","text":"Before building the operator, ensure that the DOCKER_ORG and DOCKER_REGISTRY environment variables are set correctly. These should match your Docker Hub username and the Docker registry you are using. If DOCKER_REGISTRY is unset, it defaults to docker.io . For more details, refer to the Strimzi developer guide . export DOCKER_ORG = <your_docker_hub_username> export DOCKER_REGISTRY = <your_docker_registry> # Defaults to docker.io if unset","title":"Set Docker registry variables"},{"location":"building-strimzi-for-stretch-cluster/#build-the-operator","text":"Since this is a prototype, we have not implemented test coverage yet. To avoid test compilation errors during the build process, disable test execution: DOCKER_PLATFORM = \"--platform linux/amd64\" make MVN_ARGS = '-Dmaven.test.skip=true' all Note \ud83d\udea8 If you encounter an error related to unused declared dependencies, such as Unused declared dependencies found , you might need to comment out the dependencies mentioned in the error message within the corresponding pom.xml file. \ud83d\udea8","title":"Build the operator"},{"location":"building-strimzi-for-stretch-cluster/#testing-the-operator-image-without-building","text":"If you prefer to test the operator without building it from source, you can use the pre-built image available on Docker Hub : aswinayyolath/stretchcluster:latest A detailed guide on testing the stretch cluster will be provided in a later section.","title":"Testing the operator image without building"},{"location":"setting-up-submariner/","text":"Setting up Submariner This guide explains how to install and configure Submariner to connect multiple Kubernetes clusters using either Kind or OpenShift. Installing the subctl binary Submariner provides a script to download and install the latest subctl binary: curl -Ls https://get.submariner.io | bash export PATH = $PATH :~/.local/bin echo export PATH = \\$ PATH:~/.local/bin >> ~/.profile For more installation options, refer to the Submariner documentation CNI compatibility We have tested stretch Kafka clusters with the following Container Network Interfaces (CNI): Environment CNI Kind Calico OpenShift OVN-Kubernetes","title":"Setting up Submariner"},{"location":"setting-up-submariner/#setting-up-submariner","text":"This guide explains how to install and configure Submariner to connect multiple Kubernetes clusters using either Kind or OpenShift.","title":"Setting up Submariner"},{"location":"setting-up-submariner/#installing-the-subctl-binary","text":"Submariner provides a script to download and install the latest subctl binary: curl -Ls https://get.submariner.io | bash export PATH = $PATH :~/.local/bin echo export PATH = \\$ PATH:~/.local/bin >> ~/.profile For more installation options, refer to the Submariner documentation","title":"Installing the subctl binary"},{"location":"setting-up-submariner/#cni-compatibility","text":"We have tested stretch Kafka clusters with the following Container Network Interfaces (CNI): Environment CNI Kind Calico OpenShift OVN-Kubernetes","title":"CNI compatibility"},{"location":"testing-stretch-clusters/","text":"Stretch cluster testing This section provides an overview of the basic tests performed to validate the functionality of the stretch Kafka cluster using Submariner. The goal of these tests was to ensure basic connectivity, replication, and message flow across multiple Kubernetes clusters. Pod deployment across clusters Purpose To confirm that Kafka broker pods are successfully created in multiple clusters as per the stretch configuration. Test execution \ud83d\udca0 Deployed a Kafka and KafkaNodePool CR in the central cluster. \ud83d\udca0 Ensured pods were created in the expected clusters. \ud83d\udca0 Verified pod logs for successful startup and connectivity. Results \ud83d\udd38 Kafka broker and controller pods successfully deployed in different clusters. \ud83d\udd38 Pods registered correctly with KRaft controller. \ud83d\udd38 No connectivity issues detected between brokers once running. Metadata quorum validation Purpose To verify that the Kafka controller quorum is correctly established across the clusters and that leader election functions properly. Test execution \ud83d\udca0 Deployed Kafka in a stretch cluster setup. \ud83d\udca0 Verified controller.quorum.voters configuration to ensure controllers are recognized across clusters. \ud83d\udca0 Checked logs of controller nodes to confirm leader election and quorum establishment. Results \ud83d\udd38 Kafka correctly recognized controllers in multiple clusters. \ud83d\udd38 Leader election succeeded, and controllers were able to reach consensus. \ud83d\udd38 No unexpected failures in metadata synchronization. Topic replication across clusters Purpose To ensure that topic replication functions correctly and that partitions are distributed across multiple clusters. Test execution \ud83d\udca0 Created a topic with multiple partitions and replication factor >1. \ud83d\udca0 Checked partition placement across brokers in different clusters. \ud83d\udca0 Used kafka-topics.sh --describe to verify partition replication status. Results \ud83d\udd38 Partitions were correctly placed across multiple clusters. \ud83d\udd38 Replication worked as expected, with followers keeping up with the leader. \ud83d\udd38 No significant replication lag observed. Producing and consuming messages Purpose To validate end-to-end message flow across the stretch cluster. Test execution \ud83d\udca0 Started a producer and sent messages to a replicated topic. \ud83d\udca0 Started a consumer in a different cluster and checked for message delivery. Results: \ud83d\udd38 Messages were successfully produced and consumed across clusters. \ud83d\udd38 No significant delays or dropped messages observed. \ud83d\udd38 Kafka clients could connect seamlessly across clusters.","title":"Stretch cluster testing"},{"location":"testing-stretch-clusters/#stretch-cluster-testing","text":"This section provides an overview of the basic tests performed to validate the functionality of the stretch Kafka cluster using Submariner. The goal of these tests was to ensure basic connectivity, replication, and message flow across multiple Kubernetes clusters.","title":"Stretch cluster testing"},{"location":"testing-stretch-clusters/#pod-deployment-across-clusters","text":"","title":"Pod deployment across clusters"},{"location":"testing-stretch-clusters/#purpose","text":"To confirm that Kafka broker pods are successfully created in multiple clusters as per the stretch configuration.","title":"Purpose"},{"location":"testing-stretch-clusters/#test-execution","text":"\ud83d\udca0 Deployed a Kafka and KafkaNodePool CR in the central cluster. \ud83d\udca0 Ensured pods were created in the expected clusters. \ud83d\udca0 Verified pod logs for successful startup and connectivity.","title":"Test execution"},{"location":"testing-stretch-clusters/#results","text":"\ud83d\udd38 Kafka broker and controller pods successfully deployed in different clusters. \ud83d\udd38 Pods registered correctly with KRaft controller. \ud83d\udd38 No connectivity issues detected between brokers once running.","title":"Results"},{"location":"testing-stretch-clusters/#metadata-quorum-validation","text":"","title":"Metadata quorum validation"},{"location":"testing-stretch-clusters/#purpose_1","text":"To verify that the Kafka controller quorum is correctly established across the clusters and that leader election functions properly.","title":"Purpose"},{"location":"testing-stretch-clusters/#test-execution_1","text":"\ud83d\udca0 Deployed Kafka in a stretch cluster setup. \ud83d\udca0 Verified controller.quorum.voters configuration to ensure controllers are recognized across clusters. \ud83d\udca0 Checked logs of controller nodes to confirm leader election and quorum establishment.","title":"Test execution"},{"location":"testing-stretch-clusters/#results_1","text":"\ud83d\udd38 Kafka correctly recognized controllers in multiple clusters. \ud83d\udd38 Leader election succeeded, and controllers were able to reach consensus. \ud83d\udd38 No unexpected failures in metadata synchronization.","title":"Results"},{"location":"testing-stretch-clusters/#topic-replication-across-clusters","text":"","title":"Topic replication across clusters"},{"location":"testing-stretch-clusters/#purpose_2","text":"To ensure that topic replication functions correctly and that partitions are distributed across multiple clusters.","title":"Purpose"},{"location":"testing-stretch-clusters/#test-execution_2","text":"\ud83d\udca0 Created a topic with multiple partitions and replication factor >1. \ud83d\udca0 Checked partition placement across brokers in different clusters. \ud83d\udca0 Used kafka-topics.sh --describe to verify partition replication status.","title":"Test execution"},{"location":"testing-stretch-clusters/#results_2","text":"\ud83d\udd38 Partitions were correctly placed across multiple clusters. \ud83d\udd38 Replication worked as expected, with followers keeping up with the leader. \ud83d\udd38 No significant replication lag observed.","title":"Results"},{"location":"testing-stretch-clusters/#producing-and-consuming-messages","text":"","title":"Producing and consuming messages"},{"location":"testing-stretch-clusters/#purpose_3","text":"To validate end-to-end message flow across the stretch cluster.","title":"Purpose"},{"location":"testing-stretch-clusters/#test-execution_3","text":"\ud83d\udca0 Started a producer and sent messages to a replicated topic. \ud83d\udca0 Started a consumer in a different cluster and checked for message delivery.","title":"Test execution"},{"location":"testing-stretch-clusters/#results_3","text":"\ud83d\udd38 Messages were successfully produced and consumed across clusters. \ud83d\udd38 No significant delays or dropped messages observed. \ud83d\udd38 Kafka clients could connect seamlessly across clusters.","title":"Results:"},{"location":"testing-submariner/","text":"Validating the Submariner deployment Testing connections between clusters Run the following command to check the status of connections between clusters: $ subctl show connections --kubeconfig config-str2-a Cluster \"<REDACTED>:6443\" \u2713 Showing Connections GATEWAY CLUSTER REMOTE IP NAT CABLE DRIVER SUBNETS STATUS RTT avg. worker0.<REDACTED> cluster2 10 .13.26.218 no libreswan 242 .1.0.0/16 connected 1 .2786ms worker0.<REDACTED> cluster3 10 .15.133.94 no libreswan 242 .2.0.0/16 connected 614 .899\u00b5s Diagnosing issues with the Submariner deployment Use the following command to diagnose potential issues with Submariner: $ subctl diagnose all --kubeconfig config-str2-a Cluster \"<REDACTED>:6443\" \u2713 Checking Submariner support for the Kubernetes version \u2713 Kubernetes version \"v1.28.15+ff493be\" is supported \u2713 Globalnet deployment detected - checking that globalnet CIDRs do not overlap \u2713 Checking DaemonSet \"submariner-gateway\" \u2713 Checking DaemonSet \"submariner-routeagent\" \u2713 Checking DaemonSet \"submariner-globalnet\" \u2713 Checking DaemonSet \"submariner-metrics-proxy\" \u2713 Checking Deployment \"submariner-lighthouse-agent\" \u2713 Checking Deployment \"submariner-lighthouse-coredns\" \u2713 Checking the status of all Submariner pods \u2713 Checking that gateway metrics are accessible from non-gateway nodes \u2713 Checking that globalnet metrics are accessible from non-gateway nodes \u2713 Checking Submariner support for the CNI network plugin \u2713 The detected CNI network plugin ( \"OVNKubernetes\" ) is supported \u2713 Checking OVN version \u2713 The ovn-nb database version 7 .1.0 is supported \u2713 Checking gateway connections \u2713 Checking Submariner support for the kube-proxy mode \u2713 Cluster is running with \"OVNKubernetes\" CNI which internally implements kube-proxy functionality \u2713 Checking that firewall configuration allows intra-cluster VXLAN traffic \u2717 Checking that Globalnet is correctly configured and functioning \u2717 No matching GlobalIngressIP resource found for exported service \"real/my-cluster-kafka-brokers\" \u2713 Checking that services have been exported properly Skipping inter-cluster firewall check as it requires two kubeconfigs. Please run \"subctl diagnose firewall inter-cluster\" command manually. subctl version: v0.18.0 Diagnosing intra-cluster network connectivity To check intra-cluster VXLAN firewall configuration, use: $ subctl diagnose firewall intra-cluster --kubeconfig config-str2-a Cluster \"<REDACTED>:6443\" \u2713 Checking that firewall configuration allows intra-cluster VXLAN traffic","title":"Validating the Submariner deployment"},{"location":"testing-submariner/#validating-the-submariner-deployment","text":"","title":"Validating the Submariner deployment"},{"location":"testing-submariner/#testing-connections-between-clusters","text":"Run the following command to check the status of connections between clusters: $ subctl show connections --kubeconfig config-str2-a Cluster \"<REDACTED>:6443\" \u2713 Showing Connections GATEWAY CLUSTER REMOTE IP NAT CABLE DRIVER SUBNETS STATUS RTT avg. worker0.<REDACTED> cluster2 10 .13.26.218 no libreswan 242 .1.0.0/16 connected 1 .2786ms worker0.<REDACTED> cluster3 10 .15.133.94 no libreswan 242 .2.0.0/16 connected 614 .899\u00b5s","title":"Testing connections between clusters"},{"location":"testing-submariner/#diagnosing-issues-with-the-submariner-deployment","text":"Use the following command to diagnose potential issues with Submariner: $ subctl diagnose all --kubeconfig config-str2-a Cluster \"<REDACTED>:6443\" \u2713 Checking Submariner support for the Kubernetes version \u2713 Kubernetes version \"v1.28.15+ff493be\" is supported \u2713 Globalnet deployment detected - checking that globalnet CIDRs do not overlap \u2713 Checking DaemonSet \"submariner-gateway\" \u2713 Checking DaemonSet \"submariner-routeagent\" \u2713 Checking DaemonSet \"submariner-globalnet\" \u2713 Checking DaemonSet \"submariner-metrics-proxy\" \u2713 Checking Deployment \"submariner-lighthouse-agent\" \u2713 Checking Deployment \"submariner-lighthouse-coredns\" \u2713 Checking the status of all Submariner pods \u2713 Checking that gateway metrics are accessible from non-gateway nodes \u2713 Checking that globalnet metrics are accessible from non-gateway nodes \u2713 Checking Submariner support for the CNI network plugin \u2713 The detected CNI network plugin ( \"OVNKubernetes\" ) is supported \u2713 Checking OVN version \u2713 The ovn-nb database version 7 .1.0 is supported \u2713 Checking gateway connections \u2713 Checking Submariner support for the kube-proxy mode \u2713 Cluster is running with \"OVNKubernetes\" CNI which internally implements kube-proxy functionality \u2713 Checking that firewall configuration allows intra-cluster VXLAN traffic \u2717 Checking that Globalnet is correctly configured and functioning \u2717 No matching GlobalIngressIP resource found for exported service \"real/my-cluster-kafka-brokers\" \u2713 Checking that services have been exported properly Skipping inter-cluster firewall check as it requires two kubeconfigs. Please run \"subctl diagnose firewall inter-cluster\" command manually. subctl version: v0.18.0","title":"Diagnosing issues with the Submariner deployment"},{"location":"testing-submariner/#diagnosing-intra-cluster-network-connectivity","text":"To check intra-cluster VXLAN firewall configuration, use: $ subctl diagnose firewall intra-cluster --kubeconfig config-str2-a Cluster \"<REDACTED>:6443\" \u2713 Checking that firewall configuration allows intra-cluster VXLAN traffic","title":"Diagnosing intra-cluster network connectivity"}]}